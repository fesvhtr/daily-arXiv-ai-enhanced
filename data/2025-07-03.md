<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 42]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.ET](#cs.ET) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.LG](#cs.LG) [Total: 41]
- [eess.AS](#eess.AS) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 提出了一种新型注意力机制LASAD，通过保留2D空间关系解决线性注意力在图像生成中的性能问题，并基于此构建了高效的自回归图像生成器LASADGen。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型依赖Transformer架构，计算复杂且内存开销大，线性注意力机制虽能降低复杂度，但会因无法捕捉长程依赖而降低图像生成质量。

Method: 提出线性注意力与空间感知衰减（LASAD），通过基于真实2D空间位置计算衰减因子，保留空间关系，构建LASADGen生成器。

Result: 在ImageNet上实验表明，LASADGen在图像生成性能和计算效率上达到最优。

Conclusion: LASADGen成功平衡了线性注意力的高效性与高质量图像生成所需的空间理解能力。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [2] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 提出了一种4D视频生成模型，通过跨视角点图对齐监督，实现多视角3D一致性，提升机器人对复杂环境的动态预测能力。


<details>
  <summary>Details</summary>
Motivation: 增强机器人在复杂环境中的规划和交互能力，解决现有视频生成模型在时间连贯性和几何一致性上的不足。

Method: 利用跨视角点图对齐监督训练模型，学习共享的3D场景表示，仅需RGB-D观测即可预测未来视频序列。

Result: 在模拟和真实机器人数据集上，模型生成更稳定且空间对齐的视频预测，并能恢复机器人末端执行器轨迹。

Conclusion: 该方法支持机器人操作和适应新视角，为动态场景建模提供了有效解决方案。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [3] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: 该研究提出了一种结合多源卫星影像和深度学习模型的方法，以提高滑坡识别和预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 滑坡对基础设施、经济和人类生命构成严重威胁，需要跨地理区域的准确检测和预测。

Method: 利用Sentinel-2多光谱数据和ALOS PALSAR衍生的坡度及DEM层，结合多种地理空间分析技术和深度学习分割模型（如U-Net、DeepLabV3+和Res-Net）进行滑坡检测。

Result: 研究结果表明，深度学习和多源遥感技术在构建稳健、可扩展和可转移的滑坡预测模型方面具有潜力。

Conclusion: 该框架为可靠的早期预警系统、改进的灾害风险管理和可持续土地利用规划提供了支持。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [4] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为HCNQA的3D VQA模型，通过分层监督方法解决现有答案中心监督方法的不足，确保模型发展出合理的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有3D VQA模型的答案中心监督方法仅监督最终输出，可能导致模型发展出浅层的推理捷径，且慢思考方法存在欠思考问题。

Method: 提出HCNQA模型，采用分层浓度窄化监督方法，模仿人类从广泛区域逐步聚焦到特定对象的搜索过程，通过分层监督指导模型完成三个阶段的浓度窄化。

Result: 实验结果表明，该方法能有效确保模型发展出合理的推理路径，并取得更好的性能。

Conclusion: HCNQA通过分层监督方法解决了现有方法的不足，提升了3D VQA任务的性能。

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [5] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: cp_measure是一个Python库，将CellProfiler的核心测量功能模块化，便于程序化特征提取，支持机器学习和可重复分析。


<details>
  <summary>Details</summary>
Motivation: 传统生物图像分析工具（如CellProfiler）在自动化和可重复性方面存在障碍，阻碍了机器学习工作流。

Method: 开发cp_measure库，提取CellProfiler的核心测量功能，设计为模块化、API优先的工具。

Result: cp_measure特征与CellProfiler特征高度一致，并能无缝集成Python生态系统，适用于3D成像和空间转录组学。

Conclusion: cp_measure为计算生物学中的机器学习应用提供了可扩展、可重复的图像分析解决方案。

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [6] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

TL;DR: 提出了一种高效的显著目标检测（SOD）网络设计，结合传统方法与现代CNN，通过像素差异卷积（PDC）和差异卷积重参数化（DCR）提升效率，在资源受限设备上实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有SOD模型在资源受限设备上计算成本高的问题，实现实时性能。

Method: 结合传统SOD方法和现代CNN，使用PDC编码特征对比，引入DCR减少推理计算和参数，提出STDC用于视频SOD。

Result: 模型SDNet和STDNet在效率和精度上表现优异，在Jetson Orin设备上分别达到46 FPS和150 FPS，速度和精度均优于其他轻量级模型。

Conclusion: 提出的方法在资源受限设备上实现了高效的实时SOD，为图像和视频SOD提供了新的解决方案。

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [7] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: 提出了一种基于单模态并行处理的框架，通过Holder散度和互信息处理缺失模态的脑肿瘤分割问题，在BraTS数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态MRI在脑肿瘤分割中提供互补信息，但传统方法在模态缺失时表现不佳，需解决这一问题。

Method: 利用Holder散度和互信息设计损失函数，动态调整网络参数，保持模态特异性特征。

Result: 在BraTS 2018和2020数据集上，优于现有方法，尤其在模态缺失情况下表现稳定。

Conclusion: 该框架通过动态调整和损失函数设计，有效解决了模态缺失问题，提升了分割准确性。

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [8] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 论文提出AIGVE-MACS模型，用于AI生成视频的多方面评估，结合数值评分和语言反馈，显著提升评估的全面性和与人类评价的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频评估指标仅提供数值评分，缺乏解释性，导致低可解释性和与人类评价的不一致。

Method: 提出AIGVE-MACS模型，基于AIGVE-BENCH 2数据集，结合视觉-语言模型、加权损失和动态帧采样策略。

Result: AIGVE-MACS在评分相关性和评论质量上均优于现有基线（如GPT-4o和VideoScore），并通过多代理框架提升视频生成质量53.5%。

Conclusion: AIGVE-MACS为AI生成视频的全面、人类对齐评估提供了新范式，并开源了数据集和模型。

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [9] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 该论文综述了杂草测绘领域的最新方法，填补了从数据获取到处理技术的全面分析空白，为未来研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 杂草测绘对精准管理和减少除草剂使用至关重要，但缺乏全面的文献综述，限制了领域进展。

Method: 系统分析数据获取（传感器与平台技术）、数据处理（标注与建模）和测绘技术（时空分析与决策工具），遵循PRISMA指南。

Result: 综述提供了杂草测绘领域的全面理解，支持高效、可扩展和可持续的杂草管理系统开发。

Conclusion: 该论文为未来杂草测绘研究和实践提供了重要参考，推动了该领域的进一步发展。

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [10] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种基于频域的扩散模型（\ours），用于无配对图像去雾，通过振幅残差编码器和相位校正模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的方法引入了与雾无关的内容信息，且忽略了频域中的雾相关特性。

Method: 利用扩散模型在频域中重建振幅谱，提出振幅残差编码器（ARE）和相位校正模块（PCM）。

Result: 在合成和真实数据集上优于其他最先进方法。

Conclusion: 频域扩散模型能有效利用无配对清晰数据，提升去雾效果。

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [11] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: 论文提出ET-Fuser方法，通过注意力机制结合预训练模型的任务先验，学习统一的特征表示，提升面部分析任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单任务学习中缺乏统一的特征表示，ET-Fuser旨在解决这一问题。

Method: 利用基于预训练模型任务先验的注意力机制，生成集成令牌，共享预训练编码器的互信息。

Result: 在多种面部分析任务中表现提升，特征表示显著增强。

Conclusion: ET-Fuser高效且计算成本低，为面部分析任务提供了统一的特征表示方法。

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [12] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 论文提出了一种通过将任务重新定义为铬球修复问题，从单张低动态范围（LDR）图像估计光照的简单有效方法，并利用预训练的扩散模型Stable Diffusion XL改进现有方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的HDR全景数据集，泛化能力不足，因此需要一种更通用的光照估计技术。

Method: 提出DiffusionLight，通过迭代修复生成中值铬球作为稳定的低频光照先验，并利用Exposure LoRA生成HDR光探针。进一步引入DiffusionLight-Turbo，通过Turbo LoRA直接预测迭代过程中的平均铬球，显著提升速度。

Result: 实验结果表明，该方法在多样化场景中生成逼真的光照估计，并在真实场景中表现出优越的泛化能力。

Conclusion: DiffusionLight及其加速版本DiffusionLight-Turbo提供了一种高效且通用的光照估计解决方案，尽管原始方法耗时较长，但加速版本显著提升了实用性。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [13] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: 提出了一种基于物理定律和计算模拟的新方法，直接从运动捕捉数据估计地面反作用力，提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖实验室专用的力板设备，限制了人体动力学学习的广泛应用。

Method: 结合欧拉积分方案和PD算法，从运动捕捉数据计算地面反作用力，并利用物理约束改进学习模型。

Result: 在GroundLink数据集上测试，优于基线模型，提高了地面反作用力估计和模拟根轨迹精度。

Conclusion: 该方法通过物理约束显著提升了人体动力学估计的准确性和鲁棒性。

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [14] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级方法，通过学习后照明估计映射，将中性白平衡校正转换为美学偏好校正，实现跨相机的一致性。


<details>
  <summary>Details</summary>
Motivation: 商业自动白平衡（AWB）系统通常追求美学偏好而非中性校正，且现有学习型方法难以跨相机泛化。

Method: 提出一种后照明估计映射，将中性校正转换为美学偏好校正，模型仅含约500参数，计算高效。

Result: 在771张智能手机图像数据集上表现优异，计算开销极低（0.024毫秒）。

Conclusion: 该方法首次实现美学一致性，兼容现有跨相机AWB技术，计算轻量且高效。

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [15] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: GTTA是一种通用的测试时间增强方法，适用于多种视觉和非视觉任务，通过扰动PCA子空间投影形成鲁棒集成，并通过自监督学习降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间增强方法缺乏通用性，GTTA旨在提供一种适用于多种任务的通用解决方案。

Method: GTTA通过随机扰动PCA子空间投影形成集成，并引入自监督学习阶段优化模型。

Result: 在多个数据集和任务中验证了GTTA的通用性和有效性，包括特定任务如低能见度水下视频中的鲑鱼分割。

Conclusion: GTTA是一种高效且通用的测试时间增强方法，显著提升了模型性能并降低了计算成本。

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [16] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: 提出了一种长尾分布感知路由器（LTDR），用于视觉语言混合专家模型（MoE），解决了模态特定路由和视觉尾部令牌专家激活增强的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MoE框架在视觉语言模型中依赖负载平衡机制，忽视了视觉和语言之间的固有分布差异，导致性能不足。

Method: 提出LTDR，包括分布感知路由器和针对视觉尾部令牌的专家激活增强策略。

Result: 在多个基准测试中验证了方法的有效性。

Conclusion: LTDR显著提升了视觉语言MoE模型的性能，尤其是在处理视觉尾部令牌时。

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [17] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯散射（3DGS）的物理攻击框架PGA，解决了传统方法在多视角和复杂环境中的对抗效果和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统伪装攻击方法依赖目标对象的网格先验和模拟环境，耗时且与现实存在差异，导致对抗效果和鲁棒性不足。

Method: 利用3DGS快速精确重建和逼真渲染能力，通过防止高斯相互和自遮挡，结合min-max优化调整背景，提升跨视角鲁棒性和对抗效果。

Result: 实验验证了PGA的有效性和优越性。

Conclusion: PGA在多视角和复杂环境中表现出更强的对抗效果和鲁棒性。

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [18] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: 提出了一种新的少样本奖励建模方法（Activation RMs），通过激活导向构建奖励信号，无需额外微调，优于现有方法，并在安全关键应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统奖励建模难以适应新偏好，需单独训练奖励模型。本文旨在解决这一问题，提出更灵活高效的少样本奖励建模方法。

Method: 利用激活导向（activation steering）构建奖励信号，无需额外模型微调，仅需少量监督数据。

Result: Activation RMs在标准奖励建模基准上优于现有少样本方法，并在新提出的PreferenceHack基准上表现最佳，甚至超越GPT-4o。

Conclusion: Activation RMs是一种高效、灵活的奖励建模方法，适用于安全关键应用，为模型对齐提供了新思路。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [19] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: 提出了一种名为“主动测量”的人机协作AI框架，通过重要性采样优化测量精度，减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学发现中的测量工作流缺乏准确性和统计保证，需要改进。

Method: 使用AI模型预测测量值，通过重要性采样选择样本进行人工标注，迭代优化模型和测量估计。

Result: 主动测量在多个任务中降低了估计误差，且对AI模型的准确性要求较低。

Conclusion: 主动测量框架能有效提升科学测量的精度和效率，适用于不同任务。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [20] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出了一种基于伪标签增强的音频-视觉Mamba网络（MUG），用于弱监督音频-视觉视频解析，显著提升了段级和事件级预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在弱监督和模型架构的限制下，难以同时提升段级和事件级预测性能。

Method: 通过伪标签增强生成新数据，并采用音频-视觉Mamba网络进行特征处理和交互，以增强段级事件组合的解析能力。

Result: 在LLP数据集上，MUG在所有指标上均优于现有方法（如视觉段级和音频段级指标分别提升2.1%和1.2%）。

Conclusion: MUG通过伪标签增强和Mamba网络架构，有效提升了弱监督音频-视觉视频解析的性能。

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [21] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: FixTalk框架通过EMI和EDI分别解决身份泄漏和渲染伪影问题，提升说话头生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端情况下存在身份泄漏和渲染伪影问题，影响生成质量。

Method: 提出EMI解耦身份信息与运动特征，EDI利用泄漏身份信息补充细节。

Result: 实验表明FixTalk有效减少身份泄漏和伪影，性能优于现有方法。

Conclusion: FixTalk通过创新设计解决了关键问题，为高质量说话头生成提供了新方案。

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [22] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 论文提出了一种利用标准地图（SD）信息预测车道段及其拓扑结构的方法，通过混合编码和去噪技术提升性能，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶汽车依赖高精地图（HD）的限制，通过车载传感器直接预测地图元素，并统一建模复杂的道路拓扑结构。

Method: 提出了一种网络架构，利用混合车道段编码（结合先验信息和去噪技术）和时序一致性（过去帧信息）来预测车道段及其拓扑结构。

Result: 实验证明，该方法在性能上大幅优于现有方法。

Conclusion: 通过结合先验信息和时序一致性，该方法能够更高效地在线构建高精地图，解决了复杂道路拓扑建模的挑战。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [23] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: 本文提出了一种基于多实例学习（MIL）的方法，用于胎儿腹部异常的产前超声分类，无需标准平面定位，并通过混合注意力专家模块（MoAE）、医学知识驱动的特征选择模块（MFS）和提示原型学习（PPL）提升了性能。


<details>
  <summary>Details</summary>
Motivation: 胎儿腹部畸形是严重的先天性异常，需要准确诊断以指导妊娠管理和降低死亡率。现有AI研究多关注图像级分类，较少关注病例级诊断。

Method: 采用混合注意力专家模块（MoAE）对不同平面的注意力头进行加权；提出医学知识驱动的特征选择模块（MFS）进行自监督图像标记选择；引入提示原型学习（PPL）增强MFS。

Result: 在包含2,419个病例和24,748张图像的产前腹部超声数据集上验证，性能优于现有方法。

Conclusion: 该方法在病例级诊断中表现出色，为产前超声分类提供了新思路。

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [24] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 提出了一种名为CaptionSmiths的新方法，通过量化描述长度、描述性和词汇独特性，实现图像字幕模型对生成字幕属性的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像字幕模型难以灵活控制生成字幕的属性，如描述长度和描述性，限制了其多样化应用。

Method: 量化字幕的三个属性（长度、描述性、词汇独特性）为连续标量值，并通过端点向量插值实现条件控制。

Result: 模型能平滑调整生成字幕的属性，且在词汇对齐方面优于基线，如长度控制误差降低506%。

Conclusion: CaptionSmiths通过量化属性和插值方法，实现了对字幕属性的灵活控制，提升了模型的多功能性。

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [25] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于梯度方向分析的OOD检测方法，通过短路虚假梯度来提升检测效果，同时保持ID分类性能。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中部署深度学习模型时，OOD检测至关重要。观察到ID样本和OOD样本在梯度方向上存在差异，激发了利用这一现象改进OOD检测的动机。

Method: 提出了一种推理阶段的技术，通过短路虚假梯度来降低OOD样本的置信度，同时保留ID分类性能。并引入局部一阶近似以避免重复计算。

Result: 在标准OOD基准测试中，该方法显著提升了检测效果，且计算轻量，适用于实际应用。

Conclusion: 该方法为实际应用中的OOD检测提供了一种轻量且高效的解决方案。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [26] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为DocShaDiffusion的潜在空间扩散模型，用于文档图像阴影去除，并设计了阴影软掩模生成模块（SSGM）和阴影掩模引导扩散模块（SMGDM）以解决彩色阴影问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只能处理恒定颜色背景的阴影，而忽略了彩色阴影，因此需要一种更高效的方法来去除文档图像中的彩色阴影。

Method: 通过潜在空间扩散模型（DocShaDiffusion）将阴影图像从像素空间转换到潜在空间，结合SSGM生成精确阴影掩模，并利用SMGDM引导扩散和去噪过程。此外，还提出了阴影鲁棒感知特征损失和合成数据集（SDCSRD）。

Result: 在三个公共数据集上的实验验证了该方法的优越性，优于现有技术。

Conclusion: DocShaDiffusion及其相关模块有效解决了文档图像中的彩色阴影问题，并提供了公开的代码和数据集。

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [27] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: DiffMark是一种基于扩散模型的鲁棒水印框架，通过改进训练和采样方案，结合面部图像和水印条件生成水印图像，并引入交叉信息融合模块和深度伪造抗性指导增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对安全和隐私构成威胁，现有水印方法在对抗深度伪造操作时鲁棒性不足。

Method: 提出DiffMark框架，修改扩散模型的训练和采样方案，引入交叉信息融合模块和深度伪造抗性指导。

Result: 实验证明DiffMark在典型深度伪造攻击下表现有效。

Conclusion: DiffMark通过扩散模型实现鲁棒水印，有效对抗深度伪造操作。

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [28] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: 提出了一种快速鲁棒的PCR估计器TurboReg，基于轻量级TurboClique和并行化PGS算法，显著提升了速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于最大团搜索的方法虽召回率高，但时间复杂度过高，限制了实时应用。

Method: 定义轻量级TurboClique（3-团）和并行化PGS算法，通过高SC2分数匹配对引导搜索。

Result: 在多个数据集上达到SOTA性能，速度提升显著（如208.22倍于3DMAC）。

Conclusion: TurboReg在速度和鲁棒性上均优于现有方法，适合实时应用。

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [29] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO是一个新颖的多级异常分割框架，通过粗到细的检测策略解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有像素级方法忽视空间相关性且全局阈值策略导致误判，OoDDINO旨在解决这些问题。

Method: 结合不确定性引导的检测模型和像素级分割模型，采用正交不确定性融合策略（OUAFS）和自适应双阈值网络（ADT-Net）。

Result: 在多个基准数据集上验证了OoDDINO的优越性和兼容性。

Conclusion: OoDDINO通过多级策略显著提升了异常分割的准确性。

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [30] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: NOCTIS是一种无需重新训练即可对未见物体进行实例分割的框架，结合Grounded-SAM 2和DINOv2，通过改进匹配分数计算，在BOP 2023挑战中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决无需重新训练即可对多种未见物体进行实例分割的难题。

Method: 利用Grounded-SAM 2生成物体提议和分割掩码，结合DINOv2的零样本能力生成嵌入；通过改进的匹配分数计算（包括循环阈值过滤和置信度加权）实现物体匹配。

Result: 在BOP 2023挑战的七个核心数据集上，NOCTIS表现优于现有RGB和RGB-D方法。

Conclusion: NOCTIS通过简单而强大的框架，显著提升了未见物体实例分割的性能，无需额外训练。

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [31] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为REG的方法，通过将低层图像潜在表示与预训练模型的高层类别标记纠缠，显著提高了生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如REPA）在去噪推理过程中未能充分利用判别性表示，限制了生成模型的潜力。

Method: REG方法将低层图像潜在表示与预训练模型的高层类别标记纠缠，仅需一个额外标记即可实现去噪。

Result: 在ImageNet 256×256上，SiT-XL/2 + REG实现了63倍和23倍的训练加速，且训练400K次即优于REPA训练4M次的结果。

Conclusion: REG通过语义知识主动指导图像生成，显著提升了生成模型的性能和效率。

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [32] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: 论文提出了一种快速、低功耗的甲烷检测算法（Mag1c-SAS和CEM），结合机器学习模型，显著提升了在资源有限的星载硬件上的检测效率。


<details>
  <summary>Details</summary>
Motivation: 甲烷是一种强效温室气体，通过高光谱卫星图像早期检测其泄漏有助于缓解气候变化。现有任务多为手动操作，效率低且易遗漏重要事件。

Method: 测试了快速目标检测方法（ACE、CEM），并提出Mag1c-SAS算法。结合机器学习模型（U-Net、LinkNet）评估性能。

Result: Mag1c-SAS和CEM在检测强羽流时表现良好，计算效率分别提升100倍和230倍。提出的波段选择策略进一步优化了处理速度。

Conclusion: 研究为星载甲烷检测提供了高效、低硬件需求的解决方案，代码和数据已开源。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [33] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于主动控制点的6DoF姿态跟踪方法，用于解决工业金属物体在真实环境中的反射问题。


<details>
  <summary>Details</summary>
Motivation: 工业金属物体的姿态跟踪因反射特性而具有挑战性，需要一种更有效的方法。

Method: 使用图像控制点生成边缘特征进行优化，并引入最优控制点回归方法以提高鲁棒性。

Result: 在数据集评估和实际任务中表现有效，适用于实时跟踪工业金属物体。

Conclusion: 该方法为工业金属物体的实时姿态跟踪提供了可行的解决方案，代码已开源。

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [34] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: 本文提出了一种增强高清地图构建中多模态融合方法鲁棒性的策略，包括数据增强、新型多模态融合模块和模态丢弃训练策略，并在NuScenes数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模型精度，而忽略了鲁棒性，这在现实应用中至关重要。

Method: 提出三种关键组件：数据增强、新型多模态融合模块和模态丢弃训练策略。

Result: 在NuScenes数据集上显著提升了基线方法的鲁棒性，并在干净验证集上达到了最先进性能。

Conclusion: 研究结果为开发更鲁棒可靠的高清地图构建模型提供了有价值的见解，推动了其在现实自动驾驶场景中的应用。

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [35] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: AVC-DPO是一种后训练框架，通过偏好对齐提升视频MLLMs的标题生成能力，重点关注时空动态和空间信息。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLMs在生成标题时难以根据人类偏好调整焦点。

Method: 设计增强提示，针对时空动态和空间信息，利用基础模型在不同提示条件下的响应进行偏好感知训练和对齐。

Result: 在LOVE@CVPR'25 Workshop Track 1A中表现优异，VDC基准上排名第一。

Conclusion: AVC-DPO通过偏好对齐显著提升了视频MLLMs的标题生成能力。

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [36] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: 综述分析了2018至2025年间37项关于AI害虫分类的研究，探讨了模型架构、数据集和技术挑战，指出从CNN转向混合和Transformer模型的趋势，并总结了当前的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 传统害虫监测方法效率低且难以扩展，深度学习技术（如CNN、ViT和混合模型）为自动化害虫检测提供了高效解决方案。

Method: 通过分析37项研究，按作物类型、害虫种类、模型架构、数据集和技术挑战进行分类总结。

Result: 研究发现早期研究多依赖CNN，而最新研究转向混合和Transformer模型，提高了准确性和上下文理解能力，但仍面临数据集不平衡、小害虫检测难等挑战。

Conclusion: 综述提供了该领域的结构化概述，总结了有用数据集，并指出了AI害虫监测系统的关键挑战和未来方向。

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [37] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出了一种基于ReFlow的无需训练、无需掩码的真实图像编辑方法，通过分析多模态Transformer块的中间表示，利用中步潜在特征提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: ReFlow在文本到图像生成中表现优异，但在真实图像编辑中仍存在挑战，需要一种无需训练和掩码的解决方案。

Method: 分析多模态Transformer块的中间表示，提取关键特征；利用中步潜在特征保持结构；调整注意力机制以提升编辑效果。

Result: 在两个基准测试中优于九种基线方法，人类评估显示用户偏好明显。

Conclusion: 该方法在真实图像编辑中表现优异，无需训练或掩码，具有广泛适用性。

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [38] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: 论文提出了一种结合传统方法和深度学习的规则化树冠检测方法，以提高检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 全球变暖、生物多样性丧失和空气污染等问题亟需森林监测，而自动化监测技术（如遥感与计算机视觉）是关键。

Method: 结合传统方法（特征提取与分割）和深度学习方法（树冠检测），并通过规则化后处理提升检测效果。

Result: 新方法在树冠检测数量上表现更优，并分析了其优缺点和改进空间。

Conclusion: 提出的规则化方法有效提升了树冠检测的准确性和鲁棒性，为森林监测提供了新思路。

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [39] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: 论文提出了一种名为cRID的跨模态框架，结合视觉语言模型和图注意力网络，用于检测行人数据中的PII并提升行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决街景数据作为开放数据时对行人隐私的威胁，尤其是超越面部特征的个人可识别信息（PII）。

Method: 结合大型视觉语言模型、图注意力网络和表示学习，检测文本可描述的PII线索，并提升行人重识别能力。

Result: 在Market-1501到CUHK03-np的跨数据集实验中，展示了框架的实际效果和性能提升。

Conclusion: cRID框架能有效检测语义上有意义的PII，并在行人重识别任务中表现出实用价值。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [40] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: SAM-MaGuP是一种创新的息肉分割方法，通过边界蒸馏模块和1D-2D Mamba适配器解决了弱边界问题，提升了分割精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 息肉分割在结肠镜图像中对早期结直肠癌检测至关重要，但现有方法在弱边界和泛化性方面表现不足。

Method: 结合边界蒸馏模块和1D-2D Mamba适配器，增强全局上下文交互和边界特征学习。

Result: 在五个数据集上优于现有方法，实现了更高的分割精度和鲁棒性。

Conclusion: SAM-MaGuP为息肉分割领域设定了新标准，解决了弱边界和泛化性问题。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [41] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: 本文探讨了基于姿态的数据预处理技术（归一化、插值和增强）对手语翻译（SLT）性能的影响，使用改进的T5编码器-解码器模型，实验表明这些技术能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过姿态数据预处理技术提升手语翻译系统的性能，以解决连续无注释翻译中的挑战。

Method: 采用改进的T5编码器-解码器模型处理姿态表示，并通过消融实验分析不同预处理策略的效果。

Result: 实验表明，适当的归一化、插值和增强技术能显著提高模型的鲁棒性和泛化能力，同时发现专用寄存器标记可提升性能。

Conclusion: 姿态数据预处理技术对手语翻译性能有重要影响，改进的模型架构和预处理方法能显著提升翻译效果。

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [42] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba模型的TrackingMiM架构，解决了Vision Transformer在无人机跟踪中的二次复杂度问题，实现了高效的长序列建模和实时处理。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在无人机跟踪任务中因二次复杂度问题难以实时处理数据，而现有Mamba方法未考虑时间连续性。

Method: 提出TrackingMiM，采用嵌套Mamba扫描机制，独立处理时空一致的patch token，并将模板帧编码为查询token。

Result: 在五个无人机跟踪基准测试中，TrackingMiM实现了最高精度和显著的速度提升。

Conclusion: TrackingMiM在无人机跟踪任务中表现出色，兼具高效性和实时性。

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [43] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-Félix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,Benoît Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf Hügli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning Müller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: 该论文提出了一个开源基准数据集，用于促进AI在CT分析中的泛化能力研究，并提供了基线方法和结果。


<details>
  <summary>Details</summary>
Motivation: AI在医学CT分析中因数据分布变化（如扫描仪、重建技术或剂量差异）而泛化能力差，需开发AI协调技术来解决这一问题。

Method: 使用包含1378个图像系列的数据集，涵盖不同扫描仪和设置，提出评估图像和特征稳定性的方法及开源代码。

Result: 数据集和基线方法为开发AI协调策略提供了基础，支持图像和特征层面的稳定性评估。

Conclusion: 该研究为AI在CT分析中的泛化问题提供了实用工具和基准，推动了相关技术的发展。

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [44] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 提出了一种基于IIR滤波器矩阵的方法，能去除事件相机数据中约99%的噪声，同时保留有效信号，适用于嵌入式设备。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据流存在显著噪声，影响应用效果，需高效去噪方法。

Method: 提出四种基于IIR滤波器矩阵的算法，并在多个事件数据集上测试，包括人工生成噪声和动态视觉传感器记录的噪声。

Result: 方法能去除约99%的噪声，内存占用约30KB，适用于1280x720分辨率的传感器。

Conclusion: 该方法高效去噪且适合嵌入式设备，为事件相机应用提供了实用解决方案。

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [45] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 论文提出了一种结合判别式学习和扩散生成学习的框架（IDGBR），用于改进遥感语义分割中的边界精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖判别式学习，擅长捕捉低频特征但忽视高频特征，而扩散生成模型擅长生成高频细节但低频语义推理不足。

Method: 结合判别式模型生成粗分割图，再通过扩散去噪过程细化边界，利用条件引导网络联合学习。

Result: 在五个遥感数据集上验证了IDGBR框架能一致改进不同判别式架构的粗分割结果。

Conclusion: IDGBR框架有效整合了判别式和生成式学习的优势，显著提升了边界精度。

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [46] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: SketchColour是一种基于扩散变压器（DiT）的草图到色彩转换方法，显著减少了参数和GPU内存使用，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统2D动画制作需要大量手工绘制和上色，耗时耗力。本文旨在通过自动化工具提高效率。

Method: 采用DiT架构替代传统U-Net去噪器，通过轻量级通道连接适配器和LoRA微调注入草图信息。

Result: 在SAKUGA数据集上，SketchColour在所有指标上均优于现有方法，且仅使用一半训练数据。

Conclusion: SketchColour能够生成时间一致且无显著伪影的动画，显著提升2D动画制作效率。

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [47] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出了一种基于相机参数的可控去噪框架，通过ISO、快门速度和光圈值调整去噪强度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏根据噪声水平、相机设置和用户偏好调整去噪强度的灵活性。

Method: 将ISO、快门速度和光圈值转换为向量，控制去噪网络的性能。

Result: 实验表明，该方法为去噪网络增加了可控性并提升了性能。

Conclusion: 该方法成功实现了基于相机参数的自适应去噪，代码已开源。

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [48] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: 提出了一种多模态课堂监控系统，结合睡意检测、手机使用追踪和人脸识别，提升学生注意力评估精度。


<details>
  <summary>Details</summary>
Motivation: 通过实时监测学生行为，提高课堂参与度和自动考勤效率。

Method: 使用YOLOv8检测手机和睡眠行为，LResNet Occ FC和MTCNN实现人脸识别，结合PHP和ESP32-CAM硬件。

Result: 睡眠检测mAP@50达97.42%，人脸识别准确率86.45%，手机检测mAP@50为85.89%。

Conclusion: 系统综合性能优异，适用于多样化教育环境，提升课堂监控和考勤自动化。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [49] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync提出了一种无需训练的框架，通过扩散引导实现长视频的尺度和几何一致性深度预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视频深度预测中存在尺度不一致和几何不一致的问题，主要由于滑动窗口和仅依赖2D扩散先验。

Method: 引入尺度引导和几何引导，协同优化去噪过程，确保跨窗口尺度同步和窗口内几何对齐。

Result: 实验证明DepthSync在多个数据集上显著提升了深度预测的尺度和几何一致性，尤其适用于长视频。

Conclusion: DepthSync为长视频深度预测提供了一种高效且一致的解决方案。

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [50] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: 该论文首次系统研究了深度学习人脸识别系统中的后门攻击，展示了两种针对人脸检测任务的后门攻击，并验证了大规模损失训练的特征提取器同样易受攻击。通过20种管道配置和15种攻击案例，证明了单一后门可绕过系统功能，并提出了应对措施。


<details>
  <summary>Details</summary>
Motivation: 深度学习人脸识别系统在现实场景中的广泛应用引发了安全隐患，但针对无约束系统的后门攻击研究尚属空白。

Method: 通过探索深度学习管道中的后门攻击可行性，展示了两种新型攻击（人脸生成和关键点偏移），并验证了特征提取器的脆弱性。

Result: 实验证明单一后门可绕过系统功能，20种配置和15种攻击案例均验证了攻击的有效性。

Conclusion: 论文首次系统研究了人脸识别系统的后门攻击，提出了应对措施，填补了相关领域的空白。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [51] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: POLC提出了一种感知导向的潜在编码方法，通过丰富潜在特征的语义内容，提升压缩域语义推理性能，同时减少微调参数。


<details>
  <summary>Details</summary>
Motivation: 传统基于MSE优化的图像编码模型潜在空间语义贫乏，且微调计算成本高。

Method: 提出POLC方法，通过感知导向优化潜在编码，仅需轻量适配器微调。

Result: POLC在率感知性能上与生成式图像编码方法相当，同时显著提升视觉任务性能。

Conclusion: POLC为压缩域语义推理提供了一种高效且性能优越的解决方案。

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [52] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: P3HOT框架通过结合提示引导和人类近端感知，改进了人-物接触检测任务，解决了现有模型在区域分割和类别一致性上的问题，并在多个指标上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的人-物接触检测模型在单一图像类型上表现受限，容易过度分割且难以保持区域类别一致性。

Method: P3HOT框架结合语义驱动的提示机制和人类近端感知机制，动态感知关键深度范围，并引入区域联合损失（RJLoss）和新评估指标AD-Acc。

Result: 在HOT-Annotated数据集上，P3HOT在SC-Acc.、mIoU、wIoU和AD-Acc.指标上分别提升了0.7、2.0、1.6和11.0。

Conclusion: P3HOT通过多机制融合显著提升了人-物接触检测的性能，为相关任务提供了新的解决方案。

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [53] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF是一种用于大规模场景3D重建的框架，通过分块处理和创新的采样策略，解决了传统NeRF方法的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法因内存限制难以处理大规模场景，需要一种高效且不损失质量的方法。

Method: 提出Snake-NeRF框架，采用分块处理、图像裁剪重叠和2×2 3D瓦片渐进策略，结合分段采样器避免边缘错误。

Result: 实验表明，该方法能线性时间处理大规模卫星图像，单GPU运行且质量无损失。

Conclusion: Snake-NeRF成功扩展了NeRF的应用范围，适用于大规模场景的高质量3D重建。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [54] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC是一种基础单目深度估计模型，能在多样环境条件下工作，通过无监督一致性正则化微调和空间距离约束提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础MDE模型在复杂开放世界环境中表现不佳，尤其是在光照变化、恶劣天气和传感器失真等挑战性条件下。

Method: 提出无监督一致性正则化微调范式，仅需少量未标记数据，并引入空间距离约束以学习块级相对关系。

Result: 实验证明模型在多样基准（包括恶劣天气、合成失真和通用场景）上具有零样本能力。

Conclusion: DepthAnything-AC在复杂环境条件下表现出色，为单目深度估计提供了更鲁棒的解决方案。

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [55] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: SAILViT是一种逐步特征学习增强的ViT，旨在解决ViT与LLM直接联合训练时的参数冲突和模态语义差距问题，显著提升MLLM在多模态任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViT通过图像-文本对比学习或自监督机制表现优异，但难以与LLM直接联合训练，存在参数初始化冲突和模态语义差距问题。

Method: 提出SAILViT，采用逐步特征细化方法，实现从粗到细的特征对齐和世界知识注入。

Result: SAILViT在不同参数规模、架构、训练策略和数据规模下均表现出强大的鲁棒性和泛化能力，显著提升MLLM在OpenCompass基准上的性能。

Conclusion: SAILViT有效解决了ViT与LLM联合训练的挑战，为复杂多模态交互提供了性能突破。

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [56] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出RobuSTereo框架，通过生成合成数据和改进特征提取，提升立体匹配模型在恶劣天气下的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基于学习的立体匹配模型在恶劣天气下表现不佳，主要由于训练数据稀缺和特征提取困难，限制了零样本泛化能力。

Method: 1. 提出扩散模拟管道和立体一致性模块生成高质量合成数据；2. 设计结合ConvNet和去噪Transformer的鲁棒特征编码器。

Result: 实验表明，RobuSTereo显著提升了模型在恶劣天气下的鲁棒性和泛化能力。

Conclusion: RobuSTereo通过数据增强和特征提取优化，有效解决了立体匹配模型在恶劣天气下的性能问题。

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [57] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 首届W-CODA研讨会概述，聚焦自动驾驶极端场景的下一代解决方案，结合多模态感知与理解技术。


<details>
  <summary>Details</summary>
Motivation: 探索前沿自动驾驶技术，解决极端场景下的挑战，推动完全智能、可靠的自动驾驶发展。

Method: 邀请5位学术与工业界专家分享最新进展，收集研究论文并举办双轨挑战（场景理解与生成）。

Result: 研讨会汇集多方观点与成果，为自动驾驶极端场景研究提供平台。

Conclusion: W-CODA将持续弥合前沿技术与可靠自动驾驶间的差距，推动领域发展。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [58] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SPoT提出了一种连续定位token的新策略，突破了传统网格限制，显著提升了ViT的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统tokenization方法局限于离散的patch网格，限制了模型在稀疏场景下的潜力。

Method: 提出Subpixel Placement of Tokens (SPoT)，通过连续定位token，并利用oracle-guided搜索优化位置。

Result: 实验显示，SPoT大幅减少了推理所需的token数量，同时提升了性能。

Conclusion: SPoT为ViT架构提供了灵活、高效且可解释的新方向，将稀疏性转化为优势。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [59] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: 研究探讨了端到端强化学习训练在图像目标导航任务中的有效性，分析了不同架构选择的影响，并揭示了模拟器设置对方法成功的影响。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过端到端强化学习训练解决图像目标导航任务，从而避免依赖专用图像匹配或预训练模块。

Method: 通过大规模研究分析不同架构选择（如延迟融合、通道堆叠、空间到深度投影和交叉注意力）对导航训练中相对姿态估计器的影响。

Result: 发现模拟器设置对方法成功有一定影响，但能力可部分迁移到更真实场景；导航性能与相对姿态估计性能相关。

Conclusion: 端到端强化学习训练在图像目标导航中具有一定潜力，但需注意模拟器设置的影响。

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [60] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: FACET-VLM是一个用于3D/4D面部表情识别的视觉语言框架，通过多视角学习和语义引导实现高性能。


<details>
  <summary>Details</summary>
Motivation: 3D/4D面部表情识别在情感计算中具有挑战性，但对人类行为理解、医疗监测和人机交互至关重要。

Method: 提出FACET-VLM框架，包含跨视角语义聚合（CVSA）、多视角文本引导融合（MTGF）和多视角一致性损失。

Result: 在多个基准测试中达到最先进水平，并成功扩展到4D微表情识别。

Conclusion: FACET-VLM为多模态面部表情识别提供了强大且可扩展的解决方案。

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [61] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: AdaGCD是一种自适应聚类对比学习框架，通过动态分配表示容量，解决了传统方法在未知类别数量限制下的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要预定义类别数量，无法处理真实数据的复杂性，因此提出AdaGCD以动态适应数据。

Method: 结合自适应槽注意力（AdaSlot）动态确定槽数，实现灵活聚类。

Result: 在公开和细粒度数据集上验证了有效性，尤其在利用空间局部信息方面表现优越。

Conclusion: AdaGCD通过动态表示分配，提升了开放世界场景下的类别发现能力。

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [62] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: 提出了一种基于小波域的传感器模式噪声提取方法，避免了传统图像指纹的反转步骤，提高了检测精度和处理速度。


<details>
  <summary>Details</summary>
Motivation: 传统小波去噪方法在提取传感器模式噪声时需要将指纹构建为图像，存在反转步骤的复杂性。

Method: 引入小波域指纹概念，直接在频域进行指纹提取和比较，避免反转步骤。

Result: 实验结果表明，该方法检测精度更高，处理速度显著提升。

Conclusion: 小波域指纹方法简化了提取和比较流程，具有更高的效率和准确性。

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [63] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: 论文提出了一种基于软自标记的弱监督分割方法，通过优化CRF/Potts损失的松弛形式，显著提升了仅使用部分像素标签（涂鸦）的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用硬伪标签无法表示类别不确定性或错误，因此提出软自标记以改进网络训练。

Method: 提出了一种基于软自标记的辅助损失，并系统评估了标准和新CRF松弛形式、邻域系统以及网络预测与软伪标签的连接项。同时提出了一种通用的连续子问题求解器。

Result: 软自标记方法在仅使用标准架构的情况下，显著优于更复杂的专用WSSS系统，甚至可能超越全像素级监督的效果。

Conclusion: 软自标记方法在弱监督分割中表现出色，且其通用性可应用于其他弱监督问题或系统。

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [64] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: 论文研究了剪枝对视觉模型在可解释性、无监督对象发现和人类感知对齐三个方面的影响，发现稀疏模型存在性能最佳点，但其效果高度依赖网络架构和参数规模。


<details>
  <summary>Details</summary>
Motivation: 剪枝虽能降低深度学习模型的复杂度，但其对可解释性和表示学习的影响尚不明确，因此需要深入研究。

Method: 分析不同视觉网络架构在不同稀疏度下对特征归因可解释性方法的影响，探索剪枝是否促进更简洁的结构化表示，并评估剪枝是否增强模型表示与人类感知的对齐。

Result: 发现稀疏模型存在性能最佳点（sweet spots），在这些点上模型表现出更高的可解释性、下游泛化能力和人类感知对齐，但这些效果高度依赖网络架构和参数规模。

Conclusion: 剪枝对视觉表示的影响复杂，需进一步研究其何时及如何发挥作用。

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [65] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: HOI-Dyn框架通过驱动-响应系统生成逼真的3D人-物交互，利用轻量级Transformer模型预测物体对人物动作的反应，并通过残差动力学损失提升一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将人物和物体运动独立处理，导致物理上不合理和因果不一致的行为，因此需要一种新方法来建模详细的交互动态。

Method: 提出HOI-Dyn框架，将人-物交互建模为驱动-响应系统，使用Transformer预测物体反应，并引入残差动力学损失优化训练。

Result: 实验表明，该方法提升了交互生成质量，并提供了可行的评估指标。

Conclusion: HOI-Dyn通过驱动-响应系统和动力学损失，显著改善了3D人-物交互的逼真性和一致性。

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [66] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: DeRIS框架将Referring Image Segmentation（RIS）分解为感知和认知两个模块，通过Loopback Synergy机制提升多模态认知能力，并引入数据增强解决长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有RIS框架缺乏对性能瓶颈的系统分析，尤其是多模态认知能力的不足。

Method: 提出DeRIS框架，分解RIS为感知和认知模块，引入Loopback Synergy机制和非参照样本转换数据增强。

Result: DeRIS在精确分割和图像-文本理解方面表现优异，且无需修改架构即可适应多参照场景。

Conclusion: DeRIS通过模块化设计和数据增强有效提升了RIS性能，具有广泛适用性。

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [67] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: 本文探讨了Vision Transformers（ViTs）在3D医学图像分割中的应用，特别是颅内动脉钙化（IAC）的自动量化。通过MAE框架进行自监督预训练，ViTs在IAC分割中表现优于传统监督方法，并提高了对临床场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 3D ViTs在医学图像分割中潜力巨大，但应用较少。IAC作为神经血管疾病的生物标志物，其自动量化有助于大规模风险评估。

Method: 使用MAE框架预训练ViTs，并在IST-3临床试验数据上微调，用于IAC分割。比较了不同patch大小和上采样方法的效果。

Result: 自监督ViT在Dice分数上比监督nnU-Net基线高3.2分，低patch大小和插值上采样效果更佳，且对高切片厚度更鲁棒，风险分类提升46%。

Conclusion: ViTs在医学图像分割中具有显著优势，尤其是在自监督预训练和特定架构选择下，能够显著提升性能和临床适用性。

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [68] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 论文提出两种自监督多模态预训练技术和混合模型架构，用于从SAR图像中提取冰川崩解前沿位置，显著提升了监测精度。


<details>
  <summary>Details</summary>
Motivation: 冰川冰量流失加剧，需精确监测崩解前沿以理解崩解过程。现有基于ImageNet预训练的模型因领域差异表现不佳。

Method: 提出自监督多模态预训练技术（利用新数据集SSL4SAR）和混合模型架构（Swin Transformer编码器+残差CNN解码器）。

Result: 在CaFFe基准数据集上，平均距离误差为293米，优于之前最佳模型67米；集成模型误差75米，接近人类水平（38米）。

Conclusion: 新技术显著提升冰川崩解前沿的季节性变化监测精度。

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [69] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: DisCon框架通过将离散标记作为条件信号而非生成目标，解决了连续标记建模的优化挑战，同时避免了量化带来的信息损失，显著提升了图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的进展激发了将图像编码为离散标记的兴趣，但量化过程会导致信息损失，降低图像保真度。连续标记虽能避免量化问题，但其高维无界空间增加了建模难度和生成异常的风险。

Method: 提出DisCon框架，将离散标记作为条件信号，建模连续表示在离散标记条件下的概率分布，从而规避连续标记建模的挑战。

Result: 在ImageNet 256×256生成任务中，DisCon的gFID得分为1.38，明显优于现有自回归方法。

Conclusion: DisCon通过结合离散和连续标记的优势，有效解决了图像生成中的信息损失和建模难题，为视觉生成提供了新思路。

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [70] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: 研究发现Vision Transformers（ViTs）在医学图像分类中的表示缺乏语义意义，且对微小变化敏感，可能导致不可靠的分类结果。


<details>
  <summary>Details</summary>
Motivation: 探索ViTs在医学图像任务中的表示是否具有语义意义，以及其对微小变化的鲁棒性。

Method: 使用基于投影梯度的算法分析ViTs的表示。

Result: ViTs的表示缺乏语义意义，对微小变化敏感，分类准确率可降低60%以上。

Conclusion: ViTs在医学图像分类中的表示存在根本性问题，可能影响其在安全关键系统中的部署。

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [71] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: 提出了一种名为SGP的新攻击方法，通过多尺度图像处理增强对抗样本的迁移性，显著提高了对黑盒防御模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的迁移性对深度神经网络构成安全威胁，现有方法多关注单尺度图像，限制了攻击效果。

Method: 采用高斯滤波和三种下采样方法构建多尺度样本，计算各尺度损失函数梯度的平均值以确定对抗扰动。

Result: 实验表明，SGP显著提升了对黑盒防御模型的攻击成功率，平均提升2.3%至32.6%。

Conclusion: SGP是一种可扩展性强且易于集成的输入变换方法，有效增强了对抗样本的迁移性。

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [72] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA是一种无需训练的框架，通过融合多个主题特定的LoRA模块实现多主题个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多主题个性化中需要复杂调整或联合优化的问题。

Method: 采用Full Token Tuning策略和Subject-Aware Inference，训练时模块应用于所有提示词，推理时仅激活对应主题词。

Result: 实验表明FreeLoRA在主题保真度和提示一致性上表现优异。

Conclusion: FreeLoRA提供了一种简单且通用的多主题个性化解决方案。

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [73] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 提出了一种自适应动量和解耦对比学习框架（AMD），用于提升自动驾驶中长尾轨迹预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常仅依赖基础模型的预测误差，忽略了长尾轨迹模式的多样性和不确定性，导致复杂和危险场景的预测能力不足。

Method: 结合改进的动量对比学习（MoCo-DT）和解耦对比学习（DCL）模块，设计了四种轨迹随机增强方法和在线迭代聚类策略。

Result: 在nuScenes和ETH/UCY数据集上，AMD在长尾轨迹预测和整体预测精度上均表现出色。

Conclusion: AMD框架有效提升了模型对复杂和罕见轨迹的识别能力，适应了长尾数据的分布变化。

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [74] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: 提出了一种基于多图像的超光谱重建（MI-HSR）框架，利用配备光谱滤镜的三摄像头智能手机系统，显著提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一RGB图像，导致光谱信息损失严重，重建精度受限。

Method: 采用三摄像头智能手机系统，其中两个镜头配备特定光谱滤镜，结合理论和实证分析，提出MI-HSR框架。

Result: 在新建数据集Doomer上，模型性能优于现有方法，光谱估计精度提升30%。

Conclusion: 多视角光谱滤镜结合消费级硬件可提供更准确、实用的超光谱成像解决方案。

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [75] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: 提出一种轻量级CNN框架，仅约4K参数，实现移动设备上的实时图像增强（IE），速度高达1,100 FPS，同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在资源受限平台（如移动设备）上部署的高计算和内存需求问题。

Method: 结合重参数化和增量权重优化策略，引入特征自变换模块和分层双路径注意力机制，并使用局部方差加权损失优化。

Result: 首次实现高达1,100 FPS的实时IE推理，并在多个IE任务中取得速度和性能的最佳平衡。

Conclusion: 该框架为移动设备上的实时IE提供了高效解决方案，代码已开源。

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [76] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: 提出了一种动态时序槽变换器（DTST）模块，用于解决手术视频中对象中心表示学习的挑战，并在多个手术数据库中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的异构场景（如手术视频）难以解析为有意义的一组槽，现有方法在手术视频上表现不佳。

Method: 提出动态时序槽变换器（DTST）模块，结合时序推理和预测未来槽初始化。

Result: 在多个手术数据库中实现了最先进的性能。

Conclusion: 无监督对象中心方法可应用于现实数据，并成为医疗应用中的常见工具。

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [77] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为SPRED的新框架，用于解决半监督终身行人重识别（Semi-LReID）问题，通过动态原型引导的伪标签生成和新旧知识协同净化，显著提升了未标记数据的利用效率。


<details>
  <summary>Details</summary>
Motivation: 现实场景中标注资源有限，现有方法在未标记数据利用时因噪声知识导致性能下降，因此需要一种更有效的方法。

Method: 提出SPRED框架，结合动态原型引导的伪标签生成和双知识协同净化，形成自增强循环。

Result: 在Semi-LReID基准测试中，SPRED实现了最先进的性能。

Conclusion: SPRED通过自增强循环和双知识协同，显著提升了半监督终身行人重识别的性能。

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [78] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: 论文提出了Reason50K数据集和ReasonBrain框架，用于解决复杂隐含假设指令的图像编辑问题，通过多模态大语言模型和细粒度推理线索提取模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理需要深层推理的复杂隐含假设指令，且缺乏相关数据集和细粒度细节提取机制。

Method: 提出Reason50K数据集和ReasonBrain框架，结合多模态大语言模型和细粒度推理线索提取模块（FRCE），并引入跨模态增强器（CME）减少语义损失。

Result: ReasonBrain在推理场景中表现优于现有方法，并在传统指令图像编辑任务中展现出强零样本泛化能力。

Conclusion: Reason50K和ReasonBrain为复杂指令推理的图像编辑提供了有效解决方案，具有广泛的应用潜力。

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


### [79] [Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion](https://arxiv.org/abs/2507.01909)
*Jorge Tapias Gomez,Nishant Nadkarni,Lando S. Bosma,Jue Jiang,Ergys D. Subashi,William P. Segars,James M. Balter,Mert R Sabuncu,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 论文提出了一种基于患者特异性数字孪生（DT）的方法，用于评估可变形图像配准（DIR）在胃肠道（GI）器官运动中的准确性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 临床中需要评估DIR在高度移动的GI器官中的空间准确性，但传统方法（如手动标记）难以实现。因此，提出了一种基于DT的解决方案。

Method: 通过半自动化流程，从静态3D患者扫描生成21个模拟GI运动的4D序列，并利用这些DT评估6种DIR方法的性能。

Result: 生成的DT模拟了真实的GI运动，运动幅度和Jacobian行列式与真实患者数据接近，并提供了详细的DIR性能指标和剂量映射验证。

Conclusion: 该方法为动态复杂区域的DIR工具提供了严格的测试手段，支持空间和剂量准确性的精细化评估。

Abstract: Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans using published
analytical GI motion models through a semi-automated pipeline. Eleven datasets,
including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,
and three contrast-enhanced CT scans. The motion amplitudes of the DTs were
assessed against real patient stomach motion amplitudes extracted from
independent 4D MRI datasets. The generated DTs were then used to assess six
different DIR methods using target registration error, Dice similarity
coefficient, and the 95th percentile Hausdorff distance using summary metrics
and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans
from patients treated with MR-guided radiation therapy, dose distributions were
warped and accumulated to assess dose warping errors, including evaluations of
DIR performance in both low- and high-dose regions for patient-specific error
estimation. Main results: Our proposed pipeline synthesized DTs modeling
realistic GI motion, achieving mean and maximum motion amplitudes and a mean
log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to
published real-patient gastric motion data. It also enables the extraction of
detailed quantitative DIR performance metrics and rigorous validation of dose
mapping accuracy. Significance: The pipeline enables rigorously testing DIR
tools for dynamic, anatomically complex regions enabling granular spatial and
dosimetric accuracies.

</details>


### [80] [3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP](https://arxiv.org/abs/2507.01912)
*Ranjan Sapkota,Zhichao Meng,Martin Churuvija,Xiaoqiang Du,Zenghong Ma,Manoj Karkee*

Main category: cs.CV

TL;DR: 提出了一种多季节信息融合框架，结合休眠期和生长期的RGB-D数据，通过实例分割、3D重建和模型对齐，实现果园自动化管理。


<details>
  <summary>Details</summary>
Motivation: 果园自动化中，生长期的茂密树叶遮挡了树结构，限制了机器视觉系统的能力。休眠期树叶脱落，结构更清晰可见。

Method: 使用YOLOv9-Seg进行实例分割，Kinect Fusion进行3D重建，Fast GICP进行模型对齐，融合多季节数据。

Result: YOLOv9-Seg在休眠期数据集上表现良好（MSE 0.0047，mAP@50 0.78），Kinect Fusion重建精度高（RMSE 5.23 mm），Fast GICP实现了精确的跨季节对齐（最小适应度得分0.00197）。

Conclusion: 该框架通过融合多季节数据，克服了生长期遮挡问题，提升了果园自动化操作的精度。

Abstract: In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.

</details>


### [81] [IC-Custom: Diverse Image Customization via In-Context Learning](https://arxiv.org/abs/2507.01926)
*Yaowei Li,Xiaoyu Li,Zhaoyang Zhang,Yuxuan Bian,Gan Liu,Xinyuan Li,Jiale Xu,Wenbo Hu,Yating Liu,Lingen Li,Jing Cai,Yuexian Zou,Yancheng He,Ying Shan*

Main category: cs.CV

TL;DR: IC-Custom是一个统一的图像定制框架，通过上下文学习整合位置感知和无位置定制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前图像定制方法缺乏通用框架，限制了多样化应用场景。

Method: 提出IC-Custom框架，利用DiT的多模态注意力机制和ICMA机制，结合高质量数据集。

Result: 在多个基准测试中表现优异，人类偏好提升73%，仅训练0.4%的参数量。

Conclusion: IC-Custom为工业应用提供了高效、通用的图像定制解决方案。

Abstract: Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom

</details>


### [82] [evMLP: An Efficient Event-Driven MLP Architecture for Vision](https://arxiv.org/abs/2507.01927)
*Zhentan Zheng*

Main category: cs.CV

TL;DR: 本文介绍了evMLP，一种基于事件驱动的局部更新机制的多层感知机（MLP）模型，用于图像和视频处理，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索多层感知机（MLP）在视觉模型架构中的应用，并通过事件驱动机制优化计算效率。

Method: 提出evMLP模型，利用事件驱动局部更新机制，选择性处理图像或特征图中发生变化的区域。

Result: 在ImageNet图像分类任务中表现优异，视频数据集实验显示计算成本显著降低且输出一致。

Conclusion: evMLP通过事件驱动机制有效提升计算效率，为视觉模型架构研究提供了新思路。

Abstract: Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as "events". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.

</details>


### [83] [CI-VID: A Coherent Interleaved Text-Video Dataset](https://arxiv.org/abs/2507.01938)
*Yiming Ju,Jijin Hu,Zhengxiong Luo,Haoge Deng,hanyu Zhao,Li Du,Chengwei Wu,Donglin Hao,Xinlong Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: 论文介绍了CI-VID数据集，用于支持连贯多场景视频序列的生成，超越了现有的孤立文本-视频对数据集。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集主要由孤立的文本-视频对组成，无法支持连贯多场景视频序列的建模。

Method: 提出CI-VID数据集，包含340,000个样本，每个样本包含连贯的视频片段序列及其文本描述，支持文本和视频到视频的生成。

Result: 实验表明，基于CI-VID训练的模型在生成视频序列时显著提升了准确性和内容一致性。

Conclusion: CI-VID数据集支持生成具有平滑视觉过渡和时间连贯性的故事驱动内容，具有高质量和实用价值。

Abstract: Text-to-video (T2V) generation has recently attracted considerable attention,
resulting in the development of numerous high-quality datasets that have
propelled progress in this area. However, existing public datasets are
primarily composed of isolated text-video (T-V) pairs and thus fail to support
the modeling of coherent multi-clip video sequences. To address this
limitation, we introduce CI-VID, a dataset that moves beyond isolated
text-to-video (T2V) generation toward text-and-video-to-video (TV2V)
generation, enabling models to produce coherent, multi-scene video sequences.
CI-VID contains over 340,000 samples, each featuring a coherent sequence of
video clips with text captions that capture both the individual content of each
clip and the transitions between them, enabling visually and textually grounded
generation. To further validate the effectiveness of CI-VID, we design a
comprehensive, multi-dimensional benchmark incorporating human evaluation,
VLM-based assessment, and similarity-based metrics. Experimental results
demonstrate that models trained on CI-VID exhibit significant improvements in
both accuracy and content consistency when generating video sequences. This
facilitates the creation of story-driven content with smooth visual transitions
and strong temporal coherence, underscoring the quality and practical utility
of the CI-VID dataset We release the CI-VID dataset and the accompanying code
for data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID

</details>


### [84] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: 论文提出了一种名为LongAnimation的新框架，用于解决长动画着色中的长期颜色一致性问题，结合动态全局-局部记忆模块和颜色一致性奖励机制。


<details>
  <summary>Details</summary>
Motivation: 长动画着色在动画产业中成本高昂，现有方法局限于短期着色且忽视全局信息，导致长期颜色一致性不足。

Method: 提出LongAnimation框架，包括SketchDiT、动态全局-局部记忆模块（DGLM）和颜色一致性奖励，动态融合全局历史特征与当前生成特征。

Result: 实验表明，LongAnimation在短期（14帧）和长期（平均500帧）动画着色任务中均能有效保持颜色一致性。

Conclusion: LongAnimation通过动态全局-局部范式解决了长期颜色一致性问题，为开放域动画着色提供了有效解决方案。

Abstract: Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain long-term color
consistency. In this study, we argue that ideal long-term color consistency can
be achieved through a dynamic global-local paradigm, i.e., dynamically
extracting global color-consistent features relevant to the current generation.
Specifically, we propose LongAnimation, a novel framework, which mainly
includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color
Consistency Reward. The SketchDiT captures hybrid reference features to support
the DGLM module. The DGLM module employs a long video understanding model to
dynamically compress global historical features and adaptively fuse them with
the current generation features. To refine the color consistency, we introduce
a Color Consistency Reward. During inference, we propose a color consistency
fusion to smooth the video segment transition. Extensive experiments on both
short-term (14 frames) and long-term (average 500 frames) animations show the
effectiveness of LongAnimation in maintaining short-term and long-term color
consistency for open-domain animation colorization task. The code can be found
at https://cn-makers.github.io/long_animation_web/.

</details>


### [85] [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)
*Kwai Keye Team,Biao Yang,Bin Wen,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Hao Peng,Haojie Ding,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Jin Ouyang,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yang Zhou,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zhenhua Wu,Zhenyu Li,Zhixin Ling,Ziming Li,Dehua Ma,Di Xu,Haixuan Gao,Hang Li,Jiawei Guo,Jing Wang,Lejian Ren,Muhao Wei,Qianqian Wang,Qigen Hu,Shiyao Wang,Tao Yu,Xinchen Luo,Yan Li,Yiming Liang,Yuhang Hu,Zeyi Lu,Zhuoran Yang,Zixing Zhang*

Main category: cs.CV

TL;DR: Kwai Keye-VL是一种8B参数的多模态基础模型，专注于短视频理解，通过创新的训练方法和高质量数据集实现领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在静态图像上表现优异，但在动态、信息密集的短视频理解上不足，需填补这一技术空白。

Method: 采用四阶段预训练和两阶段后训练，结合五种模式的数据混合及强化学习优化模型推理能力。

Result: Keye-VL在公开视频基准测试中达到SOTA，并在通用图像任务中保持竞争力。

Conclusion: Kwai Keye-VL成功填补短视频理解的技术空白，并通过新基准KC-MMBench验证其实际优势。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.

</details>


### [86] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: FreeMorph是一种无需调优的图像变形方法，适用于不同语义或布局的输入，通过创新设计解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练扩散模型的微调，受限于时间和语义/布局差异，FreeMorph旨在无需实例训练实现高质量图像变形。

Method: 1) 提出指导感知的球形插值设计，修改自注意力模块以解决身份丢失问题；2) 引入步进式变化趋势，混合输入图像的自注意力模块以实现可控过渡。

Result: FreeMorph在图像变形任务中表现优异，速度提升10x~50x，成为新的SOTA方法。

Conclusion: FreeMorph通过创新设计解决了调优方法的局限性，为图像变形提供了高效且高质量的解决方案。

Abstract: We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.

</details>


### [87] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: 论文评估了多模态基础模型在标准计算机视觉任务上的表现，发现它们虽不及专业模型，但作为通用模型表现尚可，且语义任务优于几何任务。


<details>
  <summary>Details</summary>
Motivation: 明确多模态基础模型在视觉理解方面的实际能力，填补现有研究的空白。

Method: 通过提示链将标准视觉任务转化为文本可提示和API兼容的任务，建立标准化评估框架。

Result: 模型在语义任务上表现较好，几何任务稍弱；GPT-4o在非推理模型中表现最佳；推理模型在几何任务上有改进。

Conclusion: 多模态基础模型在视觉任务中表现尚可，但仍有改进空间，尤其是在几何任务和提示敏感性方面。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [88] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: Locality-aware Parallel Decoding (LPD) 通过灵活并行自回归建模和局部感知生成顺序，显著加速自回归图像生成，减少生成步骤并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成依赖顺序预测，导致高延迟；现有方法并行化效果有限。

Method: 提出灵活并行自回归建模和局部感知生成顺序，支持任意生成顺序和高并行化。

Result: 在ImageNet上，生成步骤从256降至20（256×256分辨率），延迟降低至少3.4倍。

Conclusion: LPD在保持生成质量的同时，显著提升了并行化和效率。

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 论文澄清了关于大型推理模型（LRMs）是否具备真正推理能力的争议，通过改进实验方法发现LRMs在复杂任务中表现有限，但在可解问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决AI社区对LRMs是否具备真正推理能力的争议，澄清实验中的误解。

Method: 复制并改进原研究中的两个争议性基准（Towers of Hanoi和River Crossing），引入逐步提示和协作对话方法。

Result: 发现LRMs在复杂任务（如8个盘子的Towers of Hanoi）中表现受限，但在可解的River Crossing问题中表现优异。

Conclusion: LRMs是离散状态空间中的随机搜索器，未来需要在符号化、长程推理方面进一步研究。

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [90] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在医疗诊断中潜力巨大，但实际临床贡献有限，尤其在痴呆症诊断中。混合方法结合统计学习和专家知识可提升可解释性。未来研究应注重解释性、工作流适配和患者结果。


<details>
  <summary>Details</summary>
Motivation: 探索AI（尤其是LLMs）在临床诊断中的实际应用限制，特别是在痴呆症领域，以推动更有效的医疗决策支持系统。

Method: 通过范围综述分析AI在临床环境中的局限性，提出结合统计学习和专家知识的混合方法。

Result: 当前AI系统（如LLMs）在临床中表现有限，混合方法（如PEIRS和ATHENA-CDS）更适配工作流。未来需注重解释性和因果推理。

Conclusion: 未来AI决策支持应结合神经符号或混合方法，提升解释性和临床实用性，同时关注医生理解和患者结果。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [91] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: 论文探讨了基于生成式AI（GenAI）的AI代理（如LLM-Agents和MLLM-Agents）以及Agentic AI在智能制造中的潜力、挑战和未明确的定义与能力边界。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，AI代理在语义理解、复杂推理和自主决策方面的能力显著提升，但其在智能制造中的具体应用和集成仍不明确。

Method: 系统回顾了AI和AI代理技术的发展，分析了LLM-Agents、MLLM-Agents和Agentic AI的核心概念与技术进展，并探讨了其在智能制造中的应用与潜在挑战。

Result: 研究发现这些新兴AI范式在智能制造中具有广阔的应用前景，但仍需明确其定义和能力边界。

Conclusion: 研究为AI代理技术在智能制造中的进一步研究和应用提供了理论基础和实践方向。

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [92] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 本文提出了一种基于伦理风险评估的模糊规则模型，用于描述伦理决策，并通过模糊Petri网进行验证和验证。


<details>
  <summary>Details</summary>
Motivation: 由于道德领域的本体和认知复杂性，难以建立明确的道德机器评估标准。

Method: 提出了一种形式化方法，将伦理决策模型描述为模糊规则，并使用模糊Petri网进行验证和验证。

Result: 通过医学领域的案例研究验证了方法的有效性。

Conclusion: 该方法为伦理决策模型的验证提供了一种可行的途径。

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [93] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve是一个AI辅助评分平台，利用大型语言模型（LLMs）转录和评估学生手写作业，显著减少评分时间。


<details>
  <summary>Details</summary>
Motivation: 解决大规模STEM课程中手写开放式作业评分效率低下的问题。

Method: 结合LLMs转录和评估学生作业，提供评分、转录和置信度评级，支持全流程评分。

Result: 在20多所机构的课程中部署，评分30万份作业，评分时间减少65%，高置信度预测与教师评分一致率达95.4%。

Conclusion: Pensieve显著提升评分效率，同时保持高准确性。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [94] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: 提出了一种基于多代理系统和模糊逻辑的方法，用于通过SMS处理客户请求，以减少LLM幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 提高客户服务质量和响应时间是维持客户忠诚度和增加市场份额的关键，但LLM的幻觉风险是主要挑战。

Method: 结合LLM代理和模糊逻辑的多代理系统，用于处理SMS客户请求。

Result: 系统有效减少了LLM幻觉风险。

Conclusion: 多代理系统结合模糊逻辑是解决LLM幻觉问题的可行方案。

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [95] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: 提出了一种分层框架Agent-as-tool，将工具调用与推理过程分离，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究同时处理工具调用和推理过程，导致模型负担过重且效率低下。

Method: 采用分层框架，分离工具调用和推理过程，由不同代理处理。

Result: 在少量样本上微调后表现优异，Bamboogle任务中超越Search-R1。

Conclusion: Agent-as-tool框架有效减轻模型负担，提升推理效率和准确性。

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [96] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种新的TKG推理方法T3DM，通过建模分布偏移和设计高质量负采样策略，提升了模型的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有TKG推理方法在建模训练与测试样本间的分布偏移和生成高质量负样本方面存在不足，影响了模型性能。

Method: 提出T3DM方法，结合测试时训练指导的分布偏移建模和基于对抗训练的负采样策略。

Result: 实验表明T3DM在多数情况下优于现有基线方法，提供了更优且更鲁棒的结果。

Conclusion: T3DM通过改进分布偏移建模和负采样策略，显著提升了TKG推理的性能和鲁棒性。

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [97] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLMs）和自主代理从专利中挖掘并生成产品概念的方法，提出了Agent Ideate框架，实验表明代理方法在创意质量、相关性和新颖性上优于单独使用LLMs。


<details>
  <summary>Details</summary>
Motivation: 专利蕴含丰富的技术知识，但访问和解读这些信息仍具挑战性，希望通过LLMs和代理方法挖掘其创新潜力。

Method: 设计了Agent Ideate框架，结合开源LLMs和代理架构，在计算机科学、自然语言处理和材料化学三个领域进行实验。

Result: 代理方法在创意质量、相关性和新颖性上显著优于单独使用LLMs。

Conclusion: 结合LLMs与代理工作流可显著提升从专利数据生成商业创意的潜力。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [98] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: 论文研究了利用店内顾客作为配送员的集中式众包配送系统，提出了一种结合NeurADP和DDQN的动态优化策略，显著降低了配送成本。


<details>
  <summary>Details</summary>
Motivation: 针对城市地区最后一公里配送效率的需求增长，探索利用零售店顾客作为配送员的可行性。

Method: 采用马尔可夫决策过程（MDP）模型，结合NeurADP和DDQN进行动态订单分配和定价优化。

Result: 实验结果显示，NeurADP + DDQN策略比固定定价节省6.7%成本，比短视基线节省18%；灵活配送和多目的地路由进一步降低成本8%和17%。

Conclusion: 动态前瞻性策略在众包配送系统中具有显著优势，为城市物流运营商提供了实用指导。

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [99] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: 本文探讨了非单调逻辑编程中答案集语义的通用原则，质疑了传统的最小模型属性、约束单调性和基础性是否必须，并提出了基于Gelfond答案集原则的改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨传统答案集语义的通用原则是否过于严格，以及如何定义更合理的语义原则。

Method: 通过改进Gelfond答案集原则，提出基于良好支持性、默认否定最小化和认知否定最小化的新语义。

Result: 定义了新的答案集语义，并分析了其计算复杂性。

Conclusion: 结论表明传统原则有时过于严格，新提出的原则能更合理地定义答案集语义。

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [100] [Robust Multi-generation Learned Compression of Point Cloud Attribute](https://arxiv.org/abs/2507.01320)
*Xiangzuo Liu,Zhikai Liu,PengPeng Yu,Ruishan Huang,Fan Liang*

Main category: cs.MM

TL;DR: 本文首次研究了学习点云属性压缩中的多代问题，提出了三种约束方法以解决质量退化问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单次率失真优化，忽视了多代压缩中的累积失真问题。

Method: 提出了映射幂等性约束、变换可逆性约束和潜在变量一致性约束，以增强多代压缩的鲁棒性。

Result: 在Owlii和8iVFB数据集上的实验表明，所提方法能有效抑制多代损失，同时保持与基线模型相当的单次率失真性能。

Conclusion: 本文提出的约束方法为解决多代压缩问题提供了有效解决方案。

Abstract: Existing learned point cloud attribute compression methods primarily focus on
single-pass rate-distortion optimization, while overlooking the issue of
cumulative distortion in multi-generation compression scenarios. This paper,
for the first time, investigates the multi-generation issue in learned point
cloud attribute compression. We identify two primary factors contributing to
quality degradation in multi-generation compression: quantization-induced
non-idempotency and transformation irreversibility. To address the former, we
propose a Mapping Idempotency Constraint, that enables the network to learn the
complete compression-decompression mapping, enhancing its robustness to
repeated processes. To address the latter, we introduce a Transformation
Reversibility Constraint, which preserves reversible information flow via a
quantization-free training path. Further, we propose a Latent Variable
Consistency Constraint which enhances the multi-generation compression
robustness by incorporating a decompression-compression cross-generation path
and a latent variable consistency loss term. Extensive experiments conducted on
the Owlii and 8iVFB datasets verify that the proposed methods can effectively
suppress multi-generation loss while maintaining single-pass rate-distortion
performance comparable to baseline models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [101] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: MALIBU是一个用于评估基于LLM的多智能体系统中隐含社会偏见的新基准，通过场景化评估揭示偏见问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统可能强化LLM中的隐性偏见，引发公平性和代表性担忧。

Method: MALIBU通过两阶段评估：第一阶段对特定人口统计标签的响应评分，第二阶段比较不同标签的响应。

Result: 研究发现偏见缓解可能偏向边缘化群体，而非真正中立。

Conclusion: 需采用细致检测、平衡公平策略和透明评估基准来应对多智能体系统中的偏见。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [102] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: 提出了一种基于事件重叠的抽象摘要评估方法，通过比较生成摘要、参考摘要和原文的事件信息，提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 传统摘要评估依赖人工参考摘要的相似性评分，但忽略了事件信息的完整性。本文旨在通过事件重叠评估摘要质量。

Method: 利用挪威语数据集的事件标注和人工摘要，计算生成摘要、参考摘要和原文之间的事件重叠。

Result: 实验表明，该方法能更深入地分析摘要中的事件信息。

Conclusion: 事件重叠方法为抽象摘要评估提供了新的视角和更丰富的分析维度。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [103] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: 本文分析了《Nordisk familjebok》百科全书第一版和第二版中地理条目的变化，发现地理焦点从欧洲转向北美、非洲、亚洲、澳大利亚和北欧，反映了第一次世界大战和新势力崛起的影响。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过分析《Nordisk familjebok》百科全书的地理条目变化，揭示19世纪末至20世纪初瑞典社会的知识演变及其与全球事件的关联。

Method: 使用数字化文本，通过语义句子嵌入匹配条目，基于Transformer的分类器提取地理条目，并链接到Wikidata，分析地理趋势和变化。

Result: 发现地理焦点从欧洲显著转向北美、非洲、亚洲、澳大利亚和北欧，证实了第一次世界大战和新势力崛起的影响。

Conclusion: 研究展示了百科全书内容如何反映社会知识演变，为历史研究提供了新视角。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [104] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: 本文提出了一种基于xLSTM和MEGA的新框架，用于解决ABSA任务中计算效率与性能平衡的问题，通过双向mLSTM和部分翻转反向流增强局部上下文建模，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA方法在计算效率与性能之间难以平衡，xLSTM在NLP中的潜力尚未在ABSA中充分挖掘。

Method: 提出xLSTM与MEGA框架，结合双向mLSTM和部分翻转反向流（PF-mLSTM），并引入MECGAF机制动态融合前向与反向流输出。

Result: 在三个基准数据集上，MEGA优于现有方法，实现了更高的准确性和效率。

Conclusion: MEGA框架成功解决了ABSA任务中的性能与效率问题，为未来研究提供了新方向。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [105] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: 论文提出了一种去偏算法，通过从编码器表示中移除观察到的混淆信息，显著减少了文本嵌入中的偏差，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 文本序列的嵌入相似性度量可能受到无关属性（如来源或语言）的干扰，影响多语料库文本的应用效果。

Method: 采用去偏算法，从编码器表示中移除观察到的混淆信息。

Result: 去偏后，文档相似性和聚类指标在所有嵌入变体和任务中均显著提升，且不影响分布外基准性能。

Conclusion: 去偏算法有效减少了嵌入中的偏差，同时保持了其他性能，适用于多语料库场景。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [106] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型在处理非英语和非中文国家的法律问题时提供准确答案和引用的能力，提出了一种名为gAIus的架构，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLM在处理非主流语言（如波兰语）法律信息时的局限性，并提高其引用和解释能力。

Method: 方法包括提出gAIus架构，基于波兰民法典设计检索机制，并通过法律学徒入学考试题目评估效果。

Result: 结果显示，gAIus显著提升了gpt-3.5-turbo-0125的性能（419%），并超过gpt-4o，将gpt-4o-mini的得分从31%提升至86%。

Conclusion: 结论指出gAIus架构在解释性和性能上优于现有方法，并展望了未来研究和应用方向。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [107] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4在眼科中模拟临床决策的能力有限，尤其在复杂任务中表现不佳，但可能适用于教育或文档辅助。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）在眼科中模拟临床推理的潜力，特别是在糖尿病视网膜病变（DR）和青光眼筛查中的应用。

Method: 使用300张标注的眼底图像，通过结构化提示描述图像，评估GPT-4在ICDR分类、DR转诊和青光眼转诊任务中的表现，并分析临床元数据的影响。

Result: GPT-4在ICDR分类中表现中等（准确率67.5%），在DR转诊任务中表现较好（准确率82.3%），但在青光眼转诊中表现差（准确率约78%）。元数据对结果无显著影响。

Conclusion: GPT-4能模拟基本的眼科决策，但缺乏复杂任务的精确性，不适合临床使用，可能适用于教育或文档辅助。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [108] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: CARE-RAG通过冲突驱动的证据总结提升RAG系统的可靠性，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中因知识冲突导致的生成不可靠问题。

Method: 提出CARE-RAG框架，包括参数感知证据、上下文感知证据和冲突驱动总结。

Result: 在噪声或冲突证据场景下优于现有RAG基线。

Conclusion: CARE-RAG显著提升RAG系统的可信度和生成质量。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [109] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: CompactDS是一个高质量的、多样化的网络规模数据存储，显著提升了推理密集型基准测试中检索增强生成（RAG）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在推理密集型任务中表现有限，主要缺乏与预训练数据广度对齐的网络规模数据存储。

Method: 引入CompactDS，通过过滤低质量内容并结合内存近似最近邻（ANN）检索和磁盘精确搜索，实现高效检索。

Result: 在多个基准测试中，CompactDS显著提升了RAG的准确性，相对增益达10%至33%。

Conclusion: CompactDS证明了高质量、多样化数据存储的重要性，其性能优于现有复杂系统，同时保持简单性和可重现性。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [110] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA是一种新的激活稀疏化方法，通过层间正交旋转和Top-K选择，无需额外训练或基于幅度的剪枝，即可提高LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要耗时恢复训练或依赖基于幅度的剪枝，导致稀疏性不稳定和推理加速不可靠。LaRoSA旨在解决这些问题。

Method: 利用层间正交旋转将输入激活转换为更适合稀疏化的形式，并通过Top-K选择实现一致的模型级稀疏性。

Result: 在LLaMA2-7B上，40%稀疏性下，LaRoSA仅带来0.17困惑度差距，1.30倍推理加速，零样本任务准确率差距仅0.54%。

Conclusion: LaRoSA在多种LLM中表现优异，稀疏化效果稳定且推理加速显著，性能损失极小。

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [111] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 研究探讨了高级指令调优推理模型（如Deepseek-R1）在解决复杂物理问题中的表现，展示了其在SciBench基准测试中的卓越能力，并通过少样本提示进一步提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在物理推理中的复杂性和挑战，探索其在物理问题中的表现和潜力。

Method: 使用Deepseek-R1等高级指令调优推理模型，结合SciBench基准测试中的多样化物理问题，进行实验评估。

Result: 模型在复杂物理问题中达到最先进准确率，并生成独特的符号推导推理模式；少样本提示可进一步提升性能。

Conclusion: 高级推理模型在物理问题中表现出色，少样本提示仍有优化空间，展示了持续性能提升的潜力。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [112] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: LEDOM是首个纯逆向语言模型，通过逆向时序处理序列，并展示了其作为通用任务基础模型的潜力。进一步提出逆向奖励应用，显著提升数学推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索逆向语言模型作为通用基础模型的潜力，并验证其在任务中的独特能力。

Method: 训练2B和7B参数的LEDOM模型，采用逆向时序处理序列，提出逆向奖励应用以优化生成质量。

Result: LEDOM展现出独特特性，逆向奖励显著提升数学推理任务性能。

Conclusion: LEDOM具有广泛应用潜力，将公开模型和训练数据以促进研究。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [113] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种大规模偏好数据集SynPref-40M和奖励模型Skywork-Reward-V2，通过人机协同的标注流程提升数据质量，显著改进了奖励模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型在捕捉人类复杂偏好方面表现不佳，主要原因是偏好数据集的质量和范围有限。

Method: 设计了一个两阶段的人机协同标注流程，结合人类标注质量与AI扩展性，并训练了参数规模从0.6B到8B的八个奖励模型。

Result: Skywork-Reward-V2在七个主要奖励模型基准测试中达到最优性能，展示了数据规模和质量的双重重要性。

Conclusion: 通过高质量数据和人机协同标注，Skywork-Reward-V2系列显著提升了奖励模型的性能，为未来研究提供了新方向。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [114] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于注意力机制的深度学习方法，用于电子健康记录文本的信息提取和多标签疾病预测，并在MIMIC-IV数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录文本的非结构化和高维语义复杂性带来了挑战，需要一种统一建模方法。

Method: 采用基于Transformer的架构和自注意力机制进行表示学习，结合Sigmoid多标签分类器预测疾病标签，并引入上下文感知语义对齐机制。

Result: 实验表明，该方法在多项性能指标上优于现有方法，并在不同数据规模、干扰水平和模型深度下表现出强泛化能力。

Conclusion: 该框架为处理真实临床文本提供了高效算法基础，对多标签医学文本建模任务具有实际意义。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [115] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec是一种无需训练、即插即用的方法，通过利用最后一个token的logit预测下一个及下下个token，扩展检索范围，显著提升检索式推测解码的效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索式推测解码方法依赖匹配范式，难以找到准确匹配的参考token，限制了其效率和应用。

Method: LogitSpec分两步生成推测token：1) 利用最后一个token的logit预测下下个token；2) 检索与下一个及下下个token相关的参考。

Result: 实验表明，LogitSpec在多种文本生成任务中最高可实现2.61倍加速，平均每个解码步骤接受3.28个token。

Conclusion: LogitSpec通过扩展检索范围，显著提升了检索式推测解码的效率和实用性。

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [116] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 论文提出了一种基于直接偏好优化（DPO）的方法，用于个性化调整大型语言模型（LLM）的自动文本简化（ATS）系统，以满足智力障碍人士的需求。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-based ATS系统缺乏针对目标群体（如智力障碍人士）的个性化反馈，导致简化文本未能完全满足其需求。

Method: 扩展了标准的监督微调（SFT）方法，采用DPO技术，结合目标群体的偏好反馈进行后训练，并提出了一套开发个性化ATS系统的流程。

Result: 研究强调了目标群体参与设计个性化AI解决方案的重要性，并展示了DPO在提升ATS系统个性化方面的有效性。

Conclusion: 该工作为个性化包容性AI系统的开发提供了新思路，强调了目标群体参与的重要性。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [117] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: 提出了一种结合不确定性建模和微调大语言模型（LLM）的模块化框架，用于高效准确的OOS意图检测。


<details>
  <summary>Details</summary>
Motivation: 解决任务导向对话系统（TODS）中OOS意图检测的挑战，确保对未见和模糊查询的鲁棒性。

Method: 首先对现有分类器进行不确定性估计，然后利用微调的LLM对高不确定性实例进行最终决策。

Result: 在关键OOS检测基准测试中取得了最先进的结果，包括从实际部署的TODS中获取的真实数据。

Conclusion: 该方法有效平衡了计算效率和性能，结合传统方法与LLM，为OOS检测提供了高效解决方案。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [118] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: 研究发现，在立场检测任务中，外部信息（如维基百科和网络搜索）对大型语言模型（LLMs）的性能产生负面影响，导致F1分数下降高达27.9%。


<details>
  <summary>Details</summary>
Motivation: 探讨外部信息是否对LLMs在立场检测任务中有所帮助，尽管此前研究表明其对BERT等模型有提升作用。

Method: 系统评估了八种LLMs在三个数据集和12个目标上的表现，比较了使用和不使用外部信息的效果。

Result: 外部信息在多数情况下降低了性能，LLMs倾向于根据提供的信息而非文本的真实立场进行预测。

Conclusion: 研究揭示了LLMs在立场检测中的信息偏见风险，与BERT等模型的结论相反，强调了谨慎使用外部信息的必要性。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [119] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: 论文提出LUSTER系统，结合LLM和端到端强化学习，优化任务导向对话系统的任务成功率和情感响应能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型提升了语言流畅性和上下文理解，但构建高效且情感智能的任务导向对话系统仍具挑战性。

Method: 提出LUSTER系统，结合LLM和端到端强化学习，设计短期（用户情感）和长期（任务成功）奖励机制。

Result: 实验表明，结合LLM能力和结构化奖励建模能提升系统的韧性和情感响应能力。

Conclusion: LUSTER为下一代对话系统提供了实用路径，结合LLM和强化学习优化任务和情感表现。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [120] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 新数据集用于图表问答（CQA），基于可视化笔记本构建，包含真实世界的多视图图表和自然语言问题。相比现有基准，数据更贴近实际推理流程。测试显示GPT-4.1准确率仅69.3%，凸显挑战。


<details>
  <summary>Details</summary>
Motivation: 现有CQA数据集缺乏真实场景的复杂性，无法反映实际推理流程。新数据集旨在填补这一空白。

Method: 从可视化笔记本中构建数据集，包含多视图图表和自然语言问题，模拟真实分析场景。

Result: 测试中，GPT-4.1的准确率为69.3%，表明当前模型在真实CQA任务中仍有不足。

Conclusion: 新数据集为CQA提供了更真实的基准，凸显了现有模型的局限性，需进一步改进。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [121] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 论文比较了全局评分和成对比较在NLP模型评估中的优缺点，发现全局评分更可靠但可能低估某些模型，而成对比较能更好识别低分模型中的强者，但收敛较慢。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优神经语言模型的发展，NLP评估从传统全局评分转向成对比较排行榜。本文旨在比较两种方法的优劣，为模型评估策略选择提供依据。

Method: 通过合成和真实数据集的计算实验，使用标准全局指标和Bradley-Terry模型进行成对比较。

Result: 全局评分提供更可靠的总体排名，但可能低估某些模型；成对比较能有效识别低分模型中的强者，但收敛较慢。

Conclusion: 两种方法各有优劣，应根据具体需求选择评估策略。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [122] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: 研究了预训练语言模型在低资源印尼本地语言情感分析任务中的迁移能力，发现多语言模型在已见语言上表现最佳，MAD-X适配器显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索预训练语言模型在低资源印尼本地语言中的迁移能力，以支持这些语言的自然语言处理任务。

Method: 评估了零样本性能和适配器迁移方法，使用了单语印尼BERT、多语言模型（mBERT、XLM-R）和MAD-X适配器，并将目标语言分为已见、部分已见和未见三类。

Result: 多语言模型在已见语言上表现最好，部分已见次之，未见语言最差；MAD-X显著提升性能，尤其是已见和部分已见语言。

Conclusion: 模型对语言的先前接触（直接或通过相关语言）是迁移成功的最一致预测因素。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [123] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 提出AdamMeme框架，通过多智能体协作动态评估多模态大语言模型（mLLMs）对有害模因的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态数据集的评估方法无法适应模因的动态演变，需更灵活的评估框架。

Method: 采用多智能体协作框架，动态更新挑战性模因数据，深入评估mLLMs的推理能力。

Result: 实验表明AdamMeme能系统揭示不同mLLMs的性能差异，并提供细粒度分析。

Conclusion: AdamMeme为动态评估mLLMs的有害模因理解能力提供了有效工具。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [124] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 论文提出StereoBias数据集，通过联合学习偏见和刻板印象检测任务提升模型性能，实验表明联合训练显著优于单独训练。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的偏见和刻板印象可能造成危害，尤其在敏感领域如内容审核和决策中，需改进检测方法。

Method: 引入StereoBias数据集，比较编码器模型与微调解码器模型（使用QLoRA），并探索联合训练的效果。

Result: 联合训练显著提升偏见检测性能，且改进源于偏见与刻板印象的关联，而非多任务学习本身。

Conclusion: 利用刻板印象信息可构建更公平有效的AI系统。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [125] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: 论文探讨了NLP在法律文本中的应用，通过扩展数据集和利用大语言模型（LLMs）评估德国雇佣合同条款的合法性，发现法律上下文对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 法律工作的文本密集性和资源密集性为NLP研究提供了独特挑战和机会，但现有数据驱动方法缺乏可解释性和可信度，限制了其在动态法律环境中的应用。

Method: 与法律专家合作扩展数据集，利用LLMs和上下文学习评估合同条款的合法性，比较不同法律上下文（无上下文、全文法律来源、提炼版指南）对模型性能的影响。

Result: 全文法律来源对性能有中等提升，而提炼版指南显著提高了无效条款的召回率和加权F1分数（达80%），但LLMs性能仍远低于人类律师。

Conclusion: LLMs在合同合法性审查中具有辅助潜力，但当前方法仍存在局限性，需进一步改进。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [126] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: 论文探讨了分词对语言数据表示和分析结果的影响，重点研究了表情符号和同形异义词的处理方法，提出了确保数字文本在语料库中准确表示的方法。


<details>
  <summary>Details</summary>
Motivation: 分词是语料库语言学的基础步骤，但其差异会影响语言数据的表示和分析的可靠性，尤其是表情符号和同形异义词的处理。

Method: 研究提出了预处理表情符号和同形异义词的方法，以确保数字文本在语料库中的准确表示。

Result: 研究强调了理解语言和技术细节对提高语料库分析准确性的重要性，并对定量和定性研究有重要影响。

Conclusion: 为确保语料库分析的可靠性和可重复性，必须详细理解数字文本数据的语言和技术方面。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [127] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating是一个可扩展的框架，通过将高质量的英语数据质量信号转移到17种目标语言中，提升多语言大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的数据选择方法主要针对英语，而多语言数据质量对模型性能至关重要。

Method: MuRating通过聚合多个英语评分器的成对比较学习统一文档质量分数，并通过翻译将这些判断投射到多语言评估器中。

Result: 在1.2B参数的LLaMA模型上，MuRating在英语和多语言基准测试中均显著提升了准确性，尤其在知识密集型任务上表现突出。

Conclusion: MuRating为多语言数据质量评估提供了有效方法，并指出了未来在翻译保真度和内容平衡方面的研究方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [128] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: 研究发现语言模型能区分测试与部署阶段，称为评估意识，这对AI治理框架和行业承诺的可靠性构成挑战。通过线性探针实验，证实Llama-3.3-70B-Instruct模型内部能区分真实评估与部署提示，且当前安全评估已被模型视为不真实。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型的评估意识及其对AI安全和政策的影响，揭示当前评估方法的潜在缺陷。

Method: 使用线性探针分析Llama-3.3-70B-Instruct模型，区分评估与部署提示，并验证安全评估的真实性。

Result: 模型能明确区分评估与部署提示，且当前安全评估被模型视为不真实，凸显评估方法的不可靠性。

Conclusion: 需确保评估的可信度并理解模型的欺骗能力，未来可结合模型内部信息支持黑盒安全审计。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [129] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究探讨多模态AI模型在输入信息冲突时的行为，发现模型倾向于某一模态，且内部结构和注意力机制影响其偏好。


<details>
  <summary>Details</summary>
Motivation: 理解多模态模型如何处理冲突输入，为模型行为提供解释和改进方向。

Method: 通过提供冲突的视觉-语言输入（如图片与标题不符），测试模型对不同模态的偏好。

Result: 模型倾向于某一模态，且内部结构和注意力机制影响偏好；发现模态无关的“路由头”可优化性能。

Conclusion: 研究为识别和控制多模态模型在冲突信号中的行为提供了基础。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [130] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 分析了MDACE数据集，评估了可解释医疗编码系统的合理性，发现真实证据与代码描述部分一致，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 自动医疗编码可简化文档和计费流程，但需要透明性，而现有评估多限于短文本和二元设置。

Method: 对MDACE数据集进行深入分析，评估现有可解释医疗编码系统的合理性。

Result: 真实证据与代码描述部分一致，先进方法与真实证据高度重叠。

Conclusion: 提出了匹配度量，总结了成功与失败案例，并给出了开发与评估可解释医疗编码系统的建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [131] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: JSON格式在临床笔记的属性值提取中解析性最佳，结构化鲁棒性随提示优化和模型增大而提升，但在长文档和特定笔记类型中下降。


<details>
  <summary>Details</summary>
Motivation: 比较不同序列化格式（JSON、YAML、XML）在临床笔记属性值提取中的解析性，为隐私敏感的临床环境提供实用指导。

Method: 评估三种序列化格式的解析性，分析目标提示和模型大小对结构鲁棒性的影响，并进行错误模式分析。

Result: JSON解析性最高，结构鲁棒性随提示优化和模型增大而提升，但在长文档和特定笔记类型中下降。

Conclusion: JSON是临床笔记属性值提取的最佳选择，提示设计和模型规模优化可提升性能。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [132] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文提出了一种系统方法，通过分析低困惑度序列来探索大型语言模型（LLMs）如何利用和复制其训练数据，揭示了训练数据对模型输出的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，理解训练数据如何影响其输出对透明度、责任性、隐私和公平性至关重要。

Method: 通过分析低困惑度序列（模型生成的高概率文本片段），开发了一种可靠提取这些序列并追踪其训练数据来源的流程。

Result: 研究发现，大量低困惑度序列无法映射到训练数据中，而能匹配的部分则显示了逐字回忆的分布范围和性质。

Conclusion: 该方法为理解训练数据如何影响LLMs行为提供了新途径。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [133] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL 是一个统一的、生产就绪的评估框架，支持多语言（尤其是印度语言）的大语言模型评估，集成了35个基准测试，包括10个印度特定数据集。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，需要超越英语中心化基准的评估框架，以满足语言多样性地区（如印度）的需求。

Method: EKA-EVAL 集成了35个基准测试，支持分布式推理、量化和多GPU使用，提供端到端的可扩展评估套件。

Result: EKA-EVAL 是首个针对全球和印度语言模型的端到端评估套件，显著降低了多语言基准测试的门槛。

Conclusion: EKA-EVAL 是一个开源框架，旨在建立强大的多语言评估生态系统，未来计划扩展到100多个基准测试。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [134] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: DIY-MKG是一个开源的多语言学习系统，通过构建个性化词汇知识图谱和动态生成测验，解决现有语言学习工具的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语言学习工具在多语言词汇联系、个性化学习和认知负荷方面存在不足，DIY-MKG旨在解决这些问题。

Method: DIY-MKG利用LLM构建个性化词汇知识图谱，支持词汇扩展、注释和动态测验生成，并提供用户反馈机制。

Result: 评估表明，DIY-MKG在多语言词汇扩展和测验生成方面表现可靠且准确。

Conclusion: DIY-MKG是一个有效的多语言学习工具，通过个性化设计和LLM支持提升了学习体验。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [135] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: MiCoTA框架通过中间规模模型作为教师助理，提升小型语言模型的长链推理能力，显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLMs）因容量有限难以学习长链推理，而大型语言模型（LLMs）计算成本高，难以广泛部署。

Method: 引入MiCoTA框架，利用中间规模模型和中等长度推理序列，弥合容量和推理长度差距。

Result: Qwen2.5-7B和Qwen2.5-3B在多个基准测试中平均得分分别提高3.47和3.93。

Conclusion: MiCoTA为SLMs的长链推理数据蒸馏提供了新思路，未来研究可进一步探索。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [136] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 提出了一种新颖的剪枝算法，专注于高层注意力头的剪枝，并通过自适应缩放参数校准表示规模，显著提升了生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法不考虑注意力头在网络中的位置，可能导致性能下降。本文旨在通过策略性剪枝和校准提升剪枝效果。

Method: 提出了一种剪枝算法，针对高层注意力头进行剪枝，并引入自适应缩放参数以校准表示规模。

Result: 在多种LLM和27个数据集上的实验表明，该方法在生成任务中显著优于现有剪枝方法。

Conclusion: 策略性剪枝和自适应校准能有效提升剪枝效果，尤其在生成任务中表现突出。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [137] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文综述了AI在科研（AI4Research）中的应用，提出了系统分类法，指出了研究空白和未来方向，并整理了丰富的资源。


<details>
  <summary>Details</summary>
Motivation: AI在复杂领域（如逻辑推理和实验编码）的显著进展推动了其在科研中的应用，但目前缺乏全面综述，阻碍了该领域的发展。

Method: 通过系统分类法对AI4Research的五类主流任务进行分类，识别研究空白，并整理多学科应用、数据和工具资源。

Result: 提出了AI4Research的统一视角，明确了未来研究方向（如自动化实验的严谨性和可扩展性）和社会影响。

Conclusion: 本文为研究社区提供了快速获取资源的途径，有望推动AI4Research的创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [138] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 论文提出了一种名为GAPO的新方法，通过多目标优化解决LLM与多样化人类偏好对齐的问题，并引入P-GAPO以更好地满足用户需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效对齐多样化且可能冲突的人类偏好，需要一种更优的解决方案。

Method: 提出Gradient-Adaptive Policy Optimization (GAPO)，利用多梯度下降法平衡目标间的权衡，并引入P-GAPO结合用户偏好。

Result: 理论证明GAPO收敛于帕累托最优解，实验显示其在Mistral-7B上优于现有方法。

Conclusion: GAPO和P-GAPO为LLM与人类偏好对齐提供了高效且灵活的解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [139] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: 通过从教师模型中蒸馏推理痕迹，选择高质量推理样本（NaturalThoughts）能更高效提升学生模型的推理能力，优于随机采样和现有数据集。


<details>
  <summary>Details</summary>
Motivation: 研究教师模型的推理示范如何最有效地提升学生模型的推理能力。

Method: 从教师模型中精选高质量推理样本（NaturalThoughts），并分析影响推理能力蒸馏的因素，如样本效率和可扩展性。

Result: 选择需要多样化推理策略的困难样本更高效，NaturalThoughts在多个基准测试中优于现有数据集。

Conclusion: 精选高质量推理样本能显著提升学生模型的推理能力，尤其在多样化任务中表现更优。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [140] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 论文提出了一种基于决策效果的文本生成评估框架，通过衡量生成文本对人类和LLM决策结果的影响，发现传统评估方法与实际决策效果相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成（NLG）在高风险领域应用广泛，但传统评估方法（如n-gram重叠或句子合理性）与实际决策效果相关性较弱，需要更有效的评估方式。

Method: 提出决策导向的评估框架，通过市场摘要文本（如客观的早间总结和主观的收盘分析）测试人类投资者和LLM代理的决策质量。

Result: 研究发现，仅依赖摘要时，人类和LLM代理的决策表现均未显著优于随机水平；但更丰富的分析评论能显著提升人类-LLM团队的协作表现。

Conclusion: 强调评估生成文本应关注其促进人类与LLM协同决策的能力，揭示了传统内在评估指标的局限性。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [141] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 研究比较了Whisper和Wav2Vec-BERT在低资源语言Bangla上的ASR性能，发现Wav2Vec-BERT表现更优且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 评估低资源语言Bangla上两种先进ASR模型的性能，为低资源语言语音识别系统开发提供参考。

Method: 使用Mozilla Common Voice-17和OpenSLR数据集，通过微调和超参数优化比较Whisper和Wav2Vec-BERT的WER、CER、训练时间和计算效率。

Result: Wav2Vec-BERT在所有关键指标上优于Whisper，且计算资源需求更低。

Conclusion: Wav2Vec-BERT在低资源语言Bangla的ASR任务中表现更优，为低资源语言语音识别系统开发提供了实用指导。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [142] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: LLMs能进行连贯且有说服力的辩论，但缺乏对对话深层结构的理解。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在辩论中的能力及其对对话结构的理解，以揭示其作为评估者的局限性。

Method: 通过测试LLMs在辩论中的表现，并测量其对对话结构和语用背景的理解。

Result: LLMs能进行有说服力的辩论，但无法展示对深层对话结构的理解。

Conclusion: LLMs的辩论能力不依赖于对内容的真正理解，语用背景和连贯性是次要的。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [143] [Epistemic Scarcity: The Economics of Unresolvable Unknowns](https://arxiv.org/abs/2507.01483)
*Craig S Wright*

Main category: econ.GN

TL;DR: 本文通过奥地利经济学派视角，批判AI在经济协调中的局限性，并质疑主流伦理框架，强调人类自主性的重要性。


<details>
  <summary>Details</summary>
Motivation: 挑战关于AI能维持经济和认知秩序的假设，探讨其在经济协调中的不足。

Method: 运用米塞斯的先验推理和奥地利企业家理论，分析AI在经济协调中的核心功能缺失。

Result: AI无法完成经济协调的核心任务，主流伦理框架与自由主义秩序冲突。

Conclusion: AI争论关乎人类自主性，奥地利学派为对抗计算社会控制提供了唯一连贯的替代方案。

Abstract: This paper presents a praxeological analysis of artificial intelligence and
algorithmic governance, challenging assumptions about the capacity of machine
systems to sustain economic and epistemic order. Drawing on Misesian a priori
reasoning and Austrian theories of entrepreneurship, we argue that AI systems
are incapable of performing the core functions of economic coordination:
interpreting ends, discovering means, and communicating subjective value
through prices. Where neoclassical and behavioural models treat decisions as
optimisation under constraint, we frame them as purposive actions under
uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability,
and Transparency (FAT) as extensions of constructivist rationalism, which
conflict with a liberal order grounded in voluntary action and property rights.
Attempts to encode moral reasoning in algorithms reflect a misunderstanding of
ethics and economics. However complex, AI systems cannot originate norms,
interpret institutions, or bear responsibility. They remain opaque, misaligned,
and inert.
  Using the concept of epistemic scarcity, we explore how information abundance
degrades truth discernment, enabling both entrepreneurial insight and soft
totalitarianism. Our analysis ends with a civilisational claim: the debate over
AI concerns the future of human autonomy, institutional evolution, and reasoned
choice. The Austrian tradition, focused on action, subjectivity, and
spontaneous order, offers the only coherent alternative to rising computational
social control.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [144] [Evaluating LLM Agent Collusion in Double Auctions](https://arxiv.org/abs/2507.01413)
*Kushal Agrawal,Verona Teo,Juan J. Vazquez,Sudarsh Kunnavakkam,Vishak Srikanth,Andy Liu*

Main category: cs.GT

TL;DR: 研究探讨大型语言模型（LLMs）作为市场代理时可能出现的合谋行为，分析其影响因素及潜在经济伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在经济社会互动中的应用增加，识别其潜在不良行为（如合谋）变得至关重要。

Method: 通过模拟连续双重拍卖市场，研究LLM代理作为卖家的行为，分析沟通能力、模型选择和环境压力对合谋行为的影响。

Result: 直接沟通增加合谋倾向，不同模型合谋倾向不同，环境压力（如监管和紧迫性）影响合谋行为。

Conclusion: 研究强调了部署基于LLM的市场代理时需考虑的经济和伦理问题。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [145] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/abs/2507.01582)
*Jing Luo,Xinyu Yang,Jie Wei*

Main category: cs.SD

TL;DR: 该论文提出了一种名为XMVAE的模型，用于从零生成古典钢琴表演，结合了作曲家和钢琴家的双重角色。


<details>
  <summary>Details</summary>
Motivation: 古典音乐的创造力不仅来自作曲家，还来自表演者对静态乐谱的诠释。论文旨在模拟这一双重创作过程。

Method: 引入ECP表示法捕捉表演的韵律结构和表现细节，提出XMVAE模型，包含VQ-VAE分支（作曲家）和VAE分支（钢琴家），使用多尺度编码器和正交Transformer解码器。

Result: XMVAE生成的古典表演在音乐质量上优于现有模型，且作曲家分支的预训练显著提升了性能。

Conclusion: XMVAE成功模拟了作曲家和钢琴家的双重角色，为古典音乐生成提供了高质量解决方案。

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


### [146] [User-guided Generative Source Separation](https://arxiv.org/abs/2507.01339)
*Yutong Wen,Minje Kim,Paris Smaragdis*

Main category: cs.SD

TL;DR: GuideSep是一种基于扩散模型的音乐源分离方法，支持乐器无关的分离，通过波形模仿和频谱掩码提供灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于四音轨分离，缺乏灵活性，无法满足实际需求。

Method: 提出GuideSep，结合波形模仿条件和频谱掩码，采用扩散模型实现乐器无关分离。

Result: 实验证明GuideSep能高质量分离，支持更多乐器提取，用户参与提升效果。

Conclusion: GuideSep展示了扩散模型在音乐源分离中的潜力，代码和演示已公开。

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


### [147] [Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware](https://arxiv.org/abs/2507.01563)
*Marco Giordano,Stefano Giacomelli,Claudia Rinaldi,Fabio Graziosi*

Main category: cs.SD

TL;DR: 提出了一种基于E2PANNs的嵌入式紧急车辆警笛检测系统，通过优化数据集和实时部署技术，实现了低延迟、高鲁棒性的检测。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中紧急车辆警笛检测的可靠性问题，并探索低成本边缘设备的分布式声学监测网络可行性。

Method: 使用E2PANNs（基于EPANNs优化的卷积神经网络）进行二进制声音事件检测，结合定制数据集（AudioSet-EV等）和多线程推理引擎。

Result: 系统在真实音频条件下表现出低延迟检测和高鲁棒性，支持通过WebSocket实现分布式监测。

Conclusion: 该工作展示了在低成本边缘设备上部署兼容物联网的声学事件检测系统的可行性，为智能城市基础设施中的紧急车辆跟踪提供了解决方案。

Abstract: We present a full-stack emergency vehicle (EV) siren detection system
designed for real-time deployment on embedded hardware. The proposed approach
is based on E2PANNs, a fine-tuned convolutional neural network derived from
EPANNs, and optimized for binary sound event detection under urban acoustic
conditions. A key contribution is the creation of curated and semantically
structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -
developed using a custom AudioSet-Tools framework to overcome the low
reliability of standard AudioSet annotations. The system is deployed on a
Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing
a multithreaded inference engine with adaptive frame sizing, probability
smoothing, and a decision-state machine to control false positive activations.
A remote WebSocket interface provides real-time monitoring and facilitates live
demonstration capabilities. Performance is evaluated using both framewise and
event-based metrics across multiple configurations. Results show the system
achieves low-latency detection with improved robustness under realistic audio
conditions. This work demonstrates the feasibility of deploying IoS-compatible
SED solutions that can form distributed acoustic monitoring networks, enabling
collaborative emergency vehicle tracking across smart city infrastructures
through WebSocket connectivity on low-cost edge devices.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [148] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: 本文综述了提示工程在医学影像中的应用，探讨了其如何通过文本、视觉提示和可学习嵌入提升深度学习模型的性能、适应性和可解释性，同时指出了优化设计和临床部署的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中潜力巨大，但面临数据稀缺、分布偏移和任务泛化等挑战，提示方法成为提升模型性能的关键策略。

Method: 系统分析了文本指令、视觉提示和可学习嵌入等多种提示模态，及其在图像生成、分割和分类任务中的集成。

Result: 提示机制显著提高了任务准确性、鲁棒性和数据效率，减少了对人工特征工程的依赖，并增强了模型可解释性。

Conclusion: 尽管取得进展，提示设计优化、数据异质性和临床部署的可扩展性仍是挑战，未来需探索多模态提示和临床集成。

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [149] [MID-INFRARED (MIR) OCT-based inspection in industry](https://arxiv.org/abs/2507.01074)
*N. P. García-de-la-Puente,Rocío del Amor,Fernando García-Torres,Niels Møller Israelsen,Coraline Lapre,Christian Rosenberg Petersen,Ole Bang,Dominik Brouczek,Martin Schwentenwein,Kevin Neumann,Niels Benson,Valery Naranjo*

Main category: eess.IV

TL;DR: 评估中红外光学相干断层扫描（MIR OCT）系统在穿透不同材料并检测次表面不规则性方面的能力，探索其在工业无损检测中的应用。


<details>
  <summary>Details</summary>
Motivation: 为工业生产过程提供一种无损检测技术，通过MIR OCT系统检测材料次表面异常，提升质量控制能力。

Method: 在复合材料和陶瓷上进行多次采集，评估系统性能，并探索预处理和AI增强视觉算法在异常检测中的应用。

Result: 研究确定了系统的能力，并探讨了预处理和AI算法在异常检测中的潜力。

Conclusion: MIR OCT系统在工业无损检测中具有潜力，但需优化参数选择并明确其优缺点。

Abstract: This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography
(OCT) systems as a tool to penetrate different materials and detect sub-surface
irregularities. This is useful for monitoring production processes, allowing
Non-Destructive Inspection Techniques of great value to the industry. In this
exploratory study, several acquisitions are made on composite and ceramics to
know the capabilities of the system. In addition, it is assessed which
preprocessing and AI-enhanced vision algorithms can be anomaly-detection
methodologies capable of detecting abnormal zones in the analyzed objects.
Limitations and criteria for the selection of optimal parameters will be
discussed, as well as strengths and weaknesses will be highlighted.

</details>


### [150] [Classification based deep learning models for lung cancer and disease using medical images](https://arxiv.org/abs/2507.01279)
*Ahmad Chaddad,Jihao Peng,Yihang Wu*

Main category: eess.IV

TL;DR: 提出了一种名为ResNet+的新型深度卷积神经网络模型，基于ResNet框架，用于提高肺癌和肺部疾病的预测能力。通过集成ResNet-D模块和卷积注意力模块，解决了特征信息丢失问题并增强了模型泛化能力。在多个公开数据集上表现出色，准确率高达99.25%。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中的应用显著提升了肺癌预测能力，但传统CNN在降采样过程中存在特征信息丢失的问题，需要改进。

Method: 基于ResNet框架，引入ResNet-D模块改进降采样层，并加入卷积注意力模块增强特征提取和模型泛化能力。使用数据增强技术解决类别不平衡问题。

Result: 在多个公开数据集上表现优异，如LC2500数据集准确率98.14%，IQ-OTH/NCCD数据集准确率99.25%。同时计算成本更低。

Conclusion: ResNet+模型在肺癌和肺部疾病预测中优于基线模型，具有更高的准确率和更低的计算成本。

Abstract: The use of deep learning (DL) in medical image analysis has significantly
improved the ability to predict lung cancer. In this study, we introduce a
novel deep convolutional neural network (CNN) model, named ResNet+, which is
based on the established ResNet framework. This model is specifically designed
to improve the prediction of lung cancer and diseases using the images. To
address the challenge of missing feature information that occurs during the
downsampling process in CNNs, we integrate the ResNet-D module, a variant
designed to enhance feature extraction capabilities by modifying the
downsampling layers, into the traditional ResNet model. Furthermore, a
convolutional attention module was incorporated into the bottleneck layers to
enhance model generalization by allowing the network to focus on relevant
regions of the input images. We evaluated the proposed model using five public
datasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and
LCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT
$n$=425024 images). To address class imbalance, we used data augmentation
techniques to artificially increase the representation of underrepresented
classes in the training dataset. The experimental results show that ResNet+
model demonstrated remarkable accuracy/F1, reaching 98.14/98.14\% on the
LC25000 dataset and 99.25/99.13\% on the IQ-OTH/NCCD dataset. Furthermore, the
ResNet+ model saved computational cost compared to the original ResNet series
in predicting lung cancer images. The proposed model outperformed the baseline
models on publicly available datasets, achieving better performance metrics.
Our codes are publicly available at
https://github.com/AIPMLab/Graduation-2024/tree/main/Peng.

</details>


### [151] [PanTS: The Pancreatic Tumor Segmentation Dataset](https://arxiv.org/abs/2507.01291)
*Wenxuan Li,Xinze Zhou,Qi Chen,Tianyu Lin,Pedro R. A. S. Bassi,Szymon Plotka,Jaroslaw B. Cwikla,Xiaoxi Chen,Chen Ye,Zheren Zhu,Kai Ding,Heng Li,Kang Wang,Yang Yang,Yucheng Tang,Daguang Xu,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: PanTS是一个大规模胰腺CT分析数据集，包含36,390次扫描和993,000多个专家标注结构，显著提升AI模型在胰腺肿瘤检测和分割中的性能。


<details>
  <summary>Details</summary>
Motivation: 为胰腺CT分析研究提供更全面、大规模的数据资源，以解决现有公共数据集的不足。

Method: 收集来自145个医疗中心的36,390次CT扫描，并进行专家标注，涵盖胰腺肿瘤及周围24个解剖结构。

Result: 在PanTS上训练的AI模型在胰腺肿瘤检测、定位和分割任务中表现显著优于现有数据集。

Conclusion: PanTS是目前最大、最全面的胰腺CT分析数据集，为AI模型的开发和评估提供了新基准。

Abstract: PanTS is a large-scale, multi-institutional dataset curated to advance
research in pancreatic CT analysis. It contains 36,390 CT scans from 145
medical centers, with expert-validated, voxel-wise annotations of over 993,000
anatomical structures, covering pancreatic tumors, pancreas head, body, and
tail, and 24 surrounding anatomical structures such as vascular/skeletal
structures and abdominal/thoracic organs. Each scan includes metadata such as
patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,
etc. AI models trained on PanTS achieve significantly better performance in
pancreatic tumor detection, localization, and segmentation compared to those
trained on existing public datasets. Our analysis indicates that these gains
are directly attributable to the 16x larger-scale tumor annotations and
indirectly supported by the 24 additional surrounding anatomical structures. As
the largest and most comprehensive resource of its kind, PanTS offers a new
benchmark for developing and evaluating AI models in pancreatic CT analysis.

</details>


### [152] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Main category: eess.IV

TL;DR: 提出了一种名为SWinMamba的新方法，通过蛇形窗口序列和双向状态空间模型实现血管的连续分割，显著提升了血管分割的准确性和连续性。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的血管分割对疾病诊断和手术导航至关重要，但现有方法常因血管纤细和先验建模不足导致分割结果不连续。

Method: SWinMamba结合蛇形窗口序列和双向状态空间模型，通过SWToken自适应分割输入图像，BAM整合局部特征，SFFU增强特征表示。

Result: 在三个数据集上的实验表明，SWinMamba能够生成完整且连续的血管分割结果，性能优越。

Conclusion: SWinMamba通过创新建模血管连续性，显著提升了血管分割的准确性和完整性。

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [153] [Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction](https://arxiv.org/abs/2507.01326)
*Dong Liang,Xingyu Qiu,Yuzhen Li,Wei Wang,Kuanquan Wang,Suyu Dong,Gongning Luo*

Main category: eess.IV

TL;DR: 提出了一种名为S2DNets的双网络模型，通过结构和平滑性约束自监督校正MR图像的偏置场，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MR图像因设备限制存在强度不均匀性，影响诊断和分析，现有深度学习方法忽略结构和偏置场平滑性约束，导致校正结果失真。

Method: 提出S2DNets，引入分段结构约束和偏置场平滑性约束进行网络训练，有效去除不均匀强度并保留结构细节。

Result: 在临床和模拟MR数据集上的实验表明，S2DNets优于传统和深度学习方法，下游分割任务验证了其有效性。

Conclusion: S2DNets通过结构和平滑性约束显著提升了MR图像校正效果，为医学分析提供了更可靠的工具。

Abstract: MR imaging techniques are of great benefit to disease diagnosis. However, due
to the limitation of MR devices, significant intensity inhomogeneity often
exists in imaging results, which impedes both qualitative and quantitative
medical analysis. Recently, several unsupervised deep learning-based models
have been proposed for MR image improvement. However, these models merely
concentrate on global appearance learning, and neglect constraints from image
structures and smoothness of bias field, leading to distorted corrected
results. In this paper, novel structure and smoothness constrained dual
networks, named S2DNets, are proposed aiming to self-supervised bias field
correction. S2DNets introduce piece-wise structural constraints and smoothness
of bias field for network training to effectively remove non-uniform intensity
and retain much more structural details. Extensive experiments executed on both
clinical and simulated MR datasets show that the proposed model outperforms
other conventional and deep learning-based models. In addition to comparison on
visual metrics, downstream MR image segmentation tasks are also used to
evaluate the impact of the proposed model. The source code is available at:
https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.

</details>


### [154] [BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy](https://arxiv.org/abs/2507.01387)
*Ahmad Soliman,Ron Keuth,Marian Himstedt*

Main category: eess.IV

TL;DR: BronchoGAN提出了一种结合解剖学约束的图像翻译方法，通过条件GAN和中间深度图像表示，成功将不同来源的支气管镜图像转换为逼真的人体气道图像。


<details>
  <summary>Details</summary>
Motivation: 支气管镜图像数据有限，限制了深度学习模型的训练。跨域图像翻译（如虚拟支气管镜、体模等）对临床应用至关重要。

Method: 提出BronchoGAN，将解剖学约束（如支气管开口匹配）集成到条件GAN中，并使用基础模型生成的深度图像作为中间表示。

Result: 实验表明，不同来源的图像可成功转换为逼真图像，解剖学结构（如支气管开口）得到保留，FID、SSIM和Dice系数显著提升。

Conclusion: BronchoGAN通过解剖学约束和中间深度表示，解决了支气管镜图像数据不足的问题，为生成大规模逼真数据集提供了可行方案。

Abstract: The limited availability of bronchoscopy images makes image synthesis
particularly interesting for training deep learning models. Robust image
translation across different domains -- virtual bronchoscopy, phantom as well
as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This
paper proposes BronchoGAN introducing anatomical constraints for image-to-image
translation being integrated into a conditional GAN. In particular, we force
bronchial orifices to match across input and output images. We further propose
to use foundation model-generated depth images as intermediate representation
ensuring robustness across a variety of input domains establishing models with
substantially less reliance on individual training datasets. Moreover our
intermediate depth image representation allows to easily construct paired image
data for training. Our experiments showed that input images from different
domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to
images mimicking realistic human airway appearance. We demonstrated that
anatomical settings (i.e. bronchial orifices) can be robustly preserved with
our approach which is shown qualitatively and quantitatively by means of
improved FID, SSIM and dice coefficients scores. Our anatomical constraints
enabled an improvement in the Dice coefficient of up to 0.43 for synthetic
images. Through foundation models for intermediate depth representations,
bronchial orifice segmentation integrated as anatomical constraints into
conditional GANs we are able to robustly translate images from different
bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan
data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image
datasets with realistic appearance. BronchoGAN enables to bridge the gap of
missing public bronchoscopy images.

</details>


### [155] [Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling](https://arxiv.org/abs/2507.01564)
*Chia-Ming Lee,Bo-Cheng Qiu,Ting-Yao Chen,Ming-Han Sun,Fang-Ying Lin,Jung-Tse Tsai,I-An Tsai,Yu-Fan Lin,Chih-Chung Hsu*

Main category: eess.IV

TL;DR: 提出了一种基于SSFL和KDS的多源COVID-19检测方法，通过预处理和模型比较，EfficientNet表现优于Swin Transformer。


<details>
  <summary>Details</summary>
Motivation: 解决多源医学影像数据（来自四个医疗中心）的变异性问题，提高COVID-19检测的准确性。

Method: 采用SSFL框架和KDS采样，结合肺部区域提取、质量控制和自适应切片采样，选择代表性切片，并比较EfficientNet和Swin Transformer模型。

Result: EfficientNet的F1分数为94.68%，优于Swin Transformer的93.34%。

Conclusion: KDS预处理流程在多源数据中表现有效，强调了数据集平衡在多机构医学影像评估中的重要性。

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which classifies chest CT scans from four distinct medical centers. To address
multi-source variability, we employ the Spatial-Slice Feature Learning (SSFL)
framework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing
pipeline combines lung region extraction, quality control, and adaptive slice
sampling to select eight representative slices per scan. We compare
EfficientNet and Swin Transformer architectures on the validation set. The
EfficientNet model achieves an F1-score of 94.68%, compared to the Swin
Transformer's 93.34%. The results demonstrate the effectiveness of our
KDS-based pipeline on multi-source data and highlight the importance of dataset
balance in multi-institutional medical imaging evaluation.

</details>


### [156] [Robust brain age estimation from structural MRI with contrastive learning](https://arxiv.org/abs/2507.01794)
*Carlo Alberto Barbano,Benoit Dufumier,Edouard Duchesnay,Marco Grangetto,Pietro Gori*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for
characterizing normative and pathological aging. In this work, we explore
contrastive learning as a scalable and robust alternative to supervised
approaches for brain age estimation. We introduce a novel contrastive loss
function, $\mathcal{L}^{exp}$, and evaluate it across multiple public
neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four
key findings. First, scaling pre-training on diverse, multi-site data
consistently improves generalization performance, cutting external mean
absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to
site-related confounds, maintaining low scanner-predictability as training size
increases. Third, contrastive models reliably capture accelerated aging in
patients with cognitive impairment and Alzheimer's disease, as shown through
brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike
supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation
between brain age accuracy and downstream diagnostic performance, supporting
its potential as a foundation model for neuroimaging. These results position
contrastive learning as a promising direction for building generalizable and
clinically meaningful brain representations.

</details>


### [157] [Autoadaptive Medical Segment Anything Model](https://arxiv.org/abs/2507.01828)
*Tyler Ward,Meredith K. Owen,O'Kira Coleman,Brian Noehren,Abdullah-Al-Zubaer Imran*

Main category: eess.IV

TL;DR: 提出了一种名为ADA-SAM的新型多任务学习框架，用于医学图像分割，通过结合分类和分割任务提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统全监督分割模型依赖大量标注数据，成本高且耗时，需要更高效的自动分割方法。

Method: 利用辅助分类器的类别激活图指导半监督分割分支，并采用梯度反馈机制连接分割和分类任务。

Result: 在真实临床数据上验证，ADA-SAM在有限标注条件下性能优于全监督和半监督基线模型。

Conclusion: ADA-SAM是一种高效、自动化的医学图像分割方法，适用于标注数据有限的情况。

Abstract: Medical image segmentation is a key task in the imaging workflow, influencing
many image-based decisions. Traditional, fully-supervised segmentation models
rely on large amounts of labeled training data, typically obtained through
manual annotation, which can be an expensive, time-consuming, and error-prone
process. This signals a need for accurate, automatic, and annotation-efficient
methods of training these models. We propose ADA-SAM (automated,
domain-specific, and adaptive segment anything model), a novel multitask
learning framework for medical image segmentation that leverages class
activation maps from an auxiliary classifier to guide the predictions of the
semi-supervised segmentation branch, which is based on the Segment Anything
(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient
feedback mechanism to create a learnable connection between the segmentation
and classification branches by using the segmentation gradients to guide and
improve the classification predictions. We validate ADA-SAM on real-world
clinical data collected during rehabilitation trials, and demonstrate that our
proposed method outperforms both fully-supervised and semi-supervised baselines
by double digits in limited label settings. Our code is available at:
https://github.com/tbwa233/ADA-SAM.

</details>


### [158] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*Niccolò McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Main category: eess.IV

TL;DR: TANGERINE是一个开源、计算资源需求低的视觉基础模型，用于低剂量CT（LDCT）分析，支持快速适应多种疾病检测任务，显著减少训练时间和数据需求。


<details>
  <summary>Details</summary>
Motivation: 解决LDCT扫描解读中放射科医生短缺的问题，同时扩展肺癌筛查（LCS）项目以涵盖多种早期肺部疾病检测。

Method: 基于自监督学习预训练，使用超过98,000例胸部LDCT数据，采用3D掩码自编码器框架，支持快速微调。

Result: 在14种疾病分类任务中达到最先进性能，包括肺癌和多种呼吸系统疾病，且在不同临床中心表现稳健。

Conclusion: TANGERINE的开源、轻量设计为下一代医学影像工具提供了快速集成的基础，有望将LCS从单一肺癌检测转向全面的呼吸系统疾病管理。

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [159] [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](https://arxiv.org/abs/2507.01429)
*Benjamin Chen Ming Choong,Tao Luo,Cheng Liu,Bingsheng He,Wei Zhang,Joey Tianyi Zhou*

Main category: cs.ET

TL;DR: 提出了一种针对赛道存储优化的高效内存计算CNN加速器，解决了内存密度和能效问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在低资源嵌入式系统中面临数据处理挑战，赛道存储作为高效内存计算技术具有潜力，但集成内存算术电路仍存在挑战。

Method: 设计了一系列适合乘加操作的内存计算单元，并探索了赛道存储系统与CNN模型的协同设计空间。

Result: 实现了小内存占用面积，显著提升了赛道存储嵌入式系统的能效和性能。

Conclusion: 通过电路设计和系统-模型协同优化，为赛道存储嵌入式系统提供了高效解决方案。

Abstract: Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [160] [End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning](https://arxiv.org/abs/2507.01918)
*Christian Bongiorno,Efstratios Manolakis,Rosario Nunzio Mantegna*

Main category: q-fin.PM

TL;DR: 提出了一种旋转不变神经网络，用于生成全局最小方差投资组合，通过联合学习滞后变换和协方差矩阵的正则化，具有明确的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在高维股票协方差矩阵估计中的局限性，同时保持模型的可解释性和泛化能力。

Method: 设计旋转不变神经网络，联合学习滞后变换和协方差矩阵的正则化，优化未来实现的最小投资组合方差。

Result: 在2000年至2024年的样本外测试中，模型表现出更低的实现波动率、更小的最大回撤和更高的夏普比率。

Conclusion: 该模型不仅在无约束条件下表现优异，其协方差表示还可用于长约束优化器，且在实际交易框架中保持稳定。

Abstract: We develop a rotation-invariant neural network that provides the global
minimum-variance portfolio by jointly learning how to lag-transform historical
returns and how to regularise both the eigenvalues and the marginal
volatilities of large equity covariance matrices. This explicit mathematical
mapping offers clear interpretability of each module's role, so the model
cannot be regarded as a pure black-box. The architecture mirrors the analytical
form of the global minimum-variance solution yet remains agnostic to dimension,
so a single model can be calibrated on panels of a few hundred stocks and
applied, without retraining, to one thousand US equities-a cross-sectional jump
that demonstrates robust out-of-sample generalisation. The loss function is the
future realized minimum portfolio variance and is optimized end-to-end on real
daily returns. In out-of-sample tests from January 2000 to December 2024 the
estimator delivers systematically lower realised volatility, smaller maximum
drawdowns, and higher Sharpe ratios than the best analytical competitors,
including state-of-the-art non-linear shrinkage. Furthermore, although the
model is trained end-to-end to produce an unconstrained (long-short)
minimum-variance portfolio, we show that its learned covariance representation
can be used in general optimizers under long-only constraints with virtually no
loss in its performance advantage over competing estimators. These gains
persist when the strategy is executed under a highly realistic implementation
framework that models market orders at the auctions, empirical slippage,
exchange fees, and financing charges for leverage, and they remain stable
during episodes of acute market stress.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [161] [A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification](https://arxiv.org/abs/2507.01778)
*Vivek Tetarwal,Sandeep Kumar*

Main category: cs.IT

TL;DR: 本文提出了一种新型的双集成神经网络（DENN），用于基于图像特征分类清洁和脏污的太阳能板，其性能优于现有集成方法，并在Deep Solar Eye数据集上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 随着太阳能系统的普及，需要高效的维护技术以保持其最佳性能，而自动区分清洁和脏污的太阳能板是一个关键挑战。

Method: 采用双集成神经网络（DENN）框架，结合多种集成模型的优势，以提高分类准确性和鲁棒性。

Result: DENN在多种评估指标上表现优于现有集成方法，并在Deep Solar Eye数据集上实现了最先进的准确率。

Conclusion: 该研究表明混合集成学习技术在自动化太阳能板检测中具有潜力，可作为解决实际问题的可扩展方案。

Abstract: The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them into a dual framework, aimed at
improving both classification accuracy and robustness. The DENN model is
evaluated in comparison to current ensemble methods, showcasing its superior
performance across a range of assessment metrics. The proposed approach
performs the best compared to other methods and reaches state-of-the-art
accuracy on experimental results for the Deep Solar Eye dataset, effectively
serving predictive maintenance purposes in solar energy systems. It reveals the
potential of hybrid ensemble learning techniques to further advance the
prospects of automated solar panel inspections as a scalable solution to
real-world challenges.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [162] [Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem](https://arxiv.org/abs/2507.01076)
*Vanja Stojanović,Bor Pangeršič*

Main category: cs.CG

TL;DR: 本文填补了NP完全互见度（MV）问题缺乏实证分析的空白，通过实现和评估三种算法（贪婪启发式、超图近似和遗传算法），发现小图中算法表现符合理论界限，但大图中解与理论界限偏离显著。


<details>
  <summary>Details</summary>
Motivation: 解决MV问题缺乏实证分析的问题，验证算法在实际中的表现。

Method: 实现并评估三种算法：贪婪启发式、超图近似和遗传算法，使用合成图数据集进行测试。

Result: 小图中算法表现符合理论界限，大图中解与理论界限偏离显著；遗传算法在已知最优图上表现最佳。

Conclusion: 遗传算法和其他启发式方法在测试中表现最优，但大图的解质量评估仍因缺乏紧界限而复杂。

Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical
analysis on its practical behaviour despite theoretical studies. This paper
addresses this gap by implementing and evaluating three distinct algorithms - a
direct greedy heuristic, a hypergraph-based approximation, and a genetic
algorithm - on diverse synthetic graph datasets, including those with
analytically known $\mu(G)$ values and general graph models. Our results
demonstrate that for smaller graphs, the algorithms consistently achieve MV set
sizes aligning with theoretical bounds. However, for larger instances, achieved
solution sizes notably diverge from theoretical limits; this, combined with the
absence of tight bounds, complicates absolute quality assessment. Nevertheless,
validation on known optimal graphs showed the Genetic Algorithm and other
heuristics empirically performing best among tested methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [163] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Main category: cs.IR

TL;DR: 论文评估了CLIP、BLIP和LXMERT在多任务中的表现，提出了新的CDC指标，揭示了通用性与专用性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型（VLMs）在多任务中的性能一致性，为工业部署和模型开发提供指导。

Method: 通过检索、描述和推理等多样化数据集，评估模型的准确性、生成质量、效率和CDC指标。

Result: CLIP在泛化性上表现最佳（CDC: 0.92），BLIP在精选数据上表现优异，LXMERT在结构化推理中领先。

Conclusion: 结果揭示了通用性与专用性的权衡，为开发更鲁棒、灵活的任务架构提供了方向。

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [164] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Main category: cs.IR

TL;DR: M3是一个工具，通过自然语言查询简化对MIMIC-IV临床数据库的访问，降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 大型临床数据集（如MIMIC-IV）的复杂性限制了其有效使用，M3旨在降低技术障碍。

Method: M3通过单命令检索数据，利用语言模型将自然语言问题转换为SQL查询，并返回结构化结果。

Result: M3显著减少了复杂临床数据分析的时间和技能需求。

Conclusion: M3为更广泛的研究社区提供了便捷访问临床数据的途径，加速了数据转化为洞察的过程。

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [165] [A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval](https://arxiv.org/abs/2507.01058)
*Puspendu Banerjee,Aritra Mazumdar,Wazib Ansar,Saptarsi Goswami,Amlan Chakrabarti*

Main category: cs.IR

TL;DR: 该研究提出了一种结合数据科学方法（如LLM和RAG）的框架，用于高效分析加尔各答高等法院判决，包括摘要生成和相似案例检索。


<details>
  <summary>Details</summary>
Motivation: 司法资源需求增加，需提高法律文本分析效率。

Method: 使用Pegasus模型生成摘要，结合RAG技术构建向量数据库以检索相似案例。

Result: 显著提升法律案例摘要质量，高效检索相似案例，助力法律研究和决策。

Conclusion: 该框架提高了法律研究效率，便于法律从业者和学生获取关键信息，优化法律环境。

Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising
amount of legal issues, needing careful use of judicial resources. This
research presents a complex framework that leverages Data Science
methodologies, notably Large Language Models (LLM) and Retrieval-Augmented
Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta
High Court verdicts. Our framework focuses on two key aspects: first, the
creation of a robust summarization mechanism that distills complex legal texts
into concise and coherent summaries; and second, the development of an
intelligent system for retrieving similar cases, which will assist legal
professionals in research and decision making. By fine-tuning the Pegasus model
using case head note summaries, we achieve significant improvements in the
summarization of legal cases. Our two-step summarizing technique preserves
crucial legal contexts, allowing for the production of a comprehensive vector
database for RAG. The RAG-powered framework efficiently retrieves similar cases
in response to user queries, offering thorough overviews and summaries. This
technique not only improves legal research efficiency, but it also helps legal
professionals and students easily acquire and grasp key legal information,
benefiting the overall legal scenario.

</details>


### [166] [FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations](https://arxiv.org/abs/2507.01063)
*Madhav Kotecha*

Main category: cs.IR

TL;DR: 论文分析了在线约会平台的推荐系统问题，提出了改进框架以提高公平性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前约会应用的推荐系统存在算法缺陷，如流行度偏差和过滤气泡效应，限制了效果并引入有害偏见。

Method: 通过分析互惠推荐框架、公平性评估指标和行业实现，提出数学框架，包括增强相似性度量、多目标优化和公平感知算法。

Result: 当前系统性能一般（协同过滤25.1%，互惠方法28.7%），新框架在保持准确性的同时改善了公平性。

Conclusion: 研究为约会推荐系统提供了改进方向，通过多目标优化和公平算法减少偏见。

Abstract: Online dating platforms have fundamentally transformed the formation of
romantic relationships, with millions of users worldwide relying on algorithmic
matching systems to find compatible partners. However, current recommendation
systems in dating applications suffer from significant algorithmic
deficiencies, including but not limited to popularity bias, filter bubble
effects, and inadequate reciprocity modeling that limit effectiveness and
introduce harmful biases. This research integrates foundational work with
recent empirical findings to deliver a detailed analysis of dating app
recommendation systems, highlighting key issues and suggesting research-backed
solutions. Through analysis of reciprocal recommendation frameworks, fairness
evaluation metrics, and industry implementations, we demonstrate that current
systems achieve modest performance with collaborative filtering reaching 25.1\%
while reciprocal methods achieve 28.7\%. Our proposed mathematical framework
addresses these limitations through enhanced similarity measures,
multi-objective optimization, and fairness-aware algorithms that maintain
competitive accuracy while improving demographic representation to reduce
algorithmic bias.

</details>


### [167] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Main category: cs.IR

TL;DR: 该论文提出了一种基于Dense Passage Retrieval (DPR)的方法，用于从心电图的电子健康记录中检索患者队列，并设计了评估指标验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决从心电图领域的非结构化电子健康记录中检索特定患者队列的挑战。

Method: 应用DPR方法，将非结构化的心电图数据集转化为Query-Passage数据集，并设计临床场景启发的评估指标。

Result: 提出的自定义DPR嵌入模型在性能上优于传统和现成的SOTA方法。

Conclusion: 这是首次将DPR应用于心电图领域的患者队列检索，为其他医学领域提供了可借鉴的框架。

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


### [168] [Enhanced Influence-aware Group Recommendation for Online Media Propagation](https://arxiv.org/abs/2507.01616)
*Chengkun He,Xiangmin Zhou,Chen Wang,Longbing Cao,Jie Shao,Xiaodong Li,Guang Xu,Carrie Jinqiu Hu,Zahir Tari*

Main category: cs.IR

TL;DR: 提出了一种增强型影响感知群组推荐框架（EIGR），通过图提取采样策略、动态独立级联模型和两级哈希用户组索引，解决了社交图规模大、影响传播动态性及实时匹配计算开销高的问题。


<details>
  <summary>Details</summary>
Motivation: 群组推荐在社交媒体流中应用广泛，但现有方法难以应对社交图规模、影响传播动态性和实时匹配的高计算开销。

Method: 提出EIGR框架，包括图提取采样策略（GES）、动态独立级联模型（DYIC）和两级哈希用户组索引（UG-Index）。

Result: 在真实数据集上的实验表明，EIGR在效果和效率上均优于现有基线方法。

Conclusion: EIGR框架有效解决了群组推荐中的关键挑战，为实时推荐提供了高效解决方案。

Abstract: Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

</details>


### [169] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Main category: cs.IR

TL;DR: 论文提出了一种基于嵌入的检索（EBR）方法，用于补充传统分类方法在视频内容审核中的不足，通过监督对比学习框架训练嵌入模型，显著提升了性能并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在快速响应和成本效率方面存在不足，尤其是在趋势适应和紧急升级场景中。

Method: 采用监督对比学习（SCL）框架训练单模态和多模态嵌入模型，并设计了一个嵌入检索系统。

Result: 离线实验显示ROC-AUC从0.85提升至0.99，PR-AUC从0.35提升至0.95；在线实验表明动作率提高10.32%，运营成本降低80%。

Conclusion: EBR方法在性能、成本和灵活性上优于传统分类方法，适合视频内容审核。

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [170] [SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars](https://arxiv.org/abs/2507.01939)
*Xiaosheng Zhao,Yang Huang,Guirong Xue,Xiao Kong,Jifeng Liu,Xiaoyu Tang,Timothy C. Beers,Yuan-Sen Ting,A-Li Luo*

Main category: astro-ph.IM

TL;DR: SpecCLIP是一个基于大语言模型（LLM）启发的框架，用于恒星光谱分析，通过对比预训练和辅助解码器实现跨光谱应用。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型成功的启发，将类似方法扩展到恒星光谱分析，以学习鲁棒且信息丰富的嵌入，支持多样化的下游任务。

Method: 在LAMOST低分辨率和Gaia XP光谱数据上预训练，使用CLIP框架进行对比对齐，辅以解码器保留光谱信息并实现光谱类型转换。

Result: 模型在中等规模标记数据集上微调后，提升了恒星参数估计和化学丰度测定的适应性，并在外部调查数据中表现出更高的准确性和精度。

Conclusion: 对比训练的基础模型结合光谱感知解码器，可以推动精密恒星光谱学的发展。

Abstract: In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [171] [Systemic Constraints of Undecidability](https://arxiv.org/abs/2507.01036)
*Seth Bulin*

Main category: cs.FL

TL;DR: 论文提出系统性不可判定性理论，将不可计算性视为系统的结构属性而非特定函数或问题的局部特征。通过定义因果嵌入概念并证明闭包原则，表明任何参与不可判定系统计算的子系统都会继承其不可判定性。


<details>
  <summary>Details</summary>
Motivation: 重新定义不可计算性，探讨其在自然和人工系统中的普遍性，挑战通过架构创新绕过计算限制的观点。

Method: 定义因果嵌入概念，证明闭包原则，将经典结果推广到动态系统背景。

Result: 不可判定性是预测、建模和认知访问的普遍约束，无法通过架构创新规避。

Conclusion: 该框架扩展了哥德尔、图灵和柴廷的逻辑轨迹，为计算能力的拓扑结构及其与科学知识边界的关系提供了新视角。

Abstract: This paper presents a theory of systemic undecidability, reframing
incomputability as a structural property of systems rather than a localized
feature of specific functions or problems. We define a notion of causal
embedding and prove a closure principle: any subsystem that participates
functionally in the computation of an undecidable system inherits its
undecidability. This result positions undecidability as a pervasive constraint
on prediction, modeling, and epistemic access in both natural and artificial
systems. Our framework disarms oracle mimicry and challenges the view that
computational limits can be circumvented through architectural innovation. By
generalizing classical results into a dynamic systems context, this work
augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new
perspective of the topology of computability and its interrelation to the
boundaries of scientific knowledge.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [172] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Main category: cs.CR

TL;DR: 研究探讨智能家居生态系统的安全威胁，提出后量子加密和AI异常检测的有效性，但面临资源需求和可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 智能家居因物联网设备集成面临日益增长的网络安全风险，需探索有效的安全解决方案。

Method: 分类分析网络层、设备层及云与AI系统的漏洞，评估后量子加密、AI检测、区块链认证等方法。

Result: 后量子加密与AI检测有效但资源需求高；区块链认证增强安全性但需基础设施调整；现有策略缺乏可扩展性。

Conclusion: 需改进加密技术、AI威胁检测和自适应安全模型，平衡性能与效率，适应智能家居实时需求。

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


### [173] [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513)
*Beitao Chen,Xinyu Lyu,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CR

TL;DR: 本文提出了一种名为SafePTR的无训练防御框架，通过选择性剪枝有害标记来增强多模态大语言模型（MLLMs）的安全性，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉推理中表现出新的漏洞，现有防御方法未能从根本上解决多模态漏洞的根源问题。

Method: 通过分析有害多模态标记如何绕过MLLMs的安全机制，提出SafePTR框架，选择性剪枝有害标记并恢复良性特征。

Result: 实验表明，SafePTR在三个MLLMs和五个基准测试中显著提升了安全性，且不影响模型效率。

Conclusion: SafePTR是一种高效且无需训练的防御方法，能有效减少多模态越狱攻击的风险。

Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

</details>


### [174] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: 论文提出了一种隐私保护平台，帮助中小型制造商安全共享数据，以开发创新工具，并以食品晶体质量控制的图像分析为例展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 中小型制造商因竞争和隐私问题不愿共享专有数据，但需要创新工具解决实际问题。

Method: 开发隐私保护平台，制造商通过安全方法共享数据，研究人员开发工具后回传平台供使用。

Result: 开发了自动分析食品晶体大小和数量的工具，提高了效率和准确性，并通过平台实现安全使用。

Conclusion: 隐私保护平台成功解决了数据共享问题，未来可进一步扩展应用。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [175] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: 论文提出通过自然语言实现多智能体协作驾驶的意图和推理通信，以解决现有通信方式在带宽效率、信息完整性和互操作性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有协作驾驶的通信方式（如原始传感器数据、神经网络特征等）在带宽效率、信息完整性和智能体互操作性方面存在局限，且忽视了决策级融合。

Method: 采用自然语言作为通信媒介，直接传递意图、推理和决策，实现从感知数据共享到主动协调的转变。

Result: 自然语言通信在语义密度、带宽适应性和异构平台兼容性上表现优越，提升了协作驾驶的安全性、效率和透明度。

Conclusion: 自然语言通信为多智能体协作驾驶提供了一种更高效、灵活和透明的解决方案，推动了智能交通系统的发展。

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [176] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/abs/2507.01378)
*Ziyao Wang,Rongpeng Li,Sizhao Li,Yuming Xiang,Haiping Wang,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种基于角色自适应LLM驱动的导航算法RALLY，通过结合语义决策框架、动态角色异构机制和角色值混合网络，解决了传统多智能体强化学习和静态LLM框架的局限性，显著提升了任务覆盖率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习（MARL）在数值通信语义鸿沟和角色同质性上表现不佳，而基于大语言模型（LLM）的静态框架缺乏在线学习能力。因此，需要一种新方法结合两者的优势。

Method: 提出RALLY算法，包括LLM驱动的语义决策框架、动态角色异构机制和基于RMIX的角色选择策略，结合离线先验和在线策略进行半离线训练。

Result: 在MPE环境和SITL平台上的实验表明，RALLY在任务覆盖率、收敛速度和泛化能力上优于传统方法。

Conclusion: RALLY展示了在多无人机系统中协作导航的强大潜力，为智能控制提供了新思路。

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


### [177] [Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture](https://arxiv.org/abs/2507.01701)
*Bochen Han,Songmao Zhang*

Main category: cs.MA

TL;DR: 提出一种将黑板架构融入LLM多代理系统的方法，实现信息共享、动态代理选择和高效问题解决。


<details>
  <summary>Details</summary>
Motivation: 解决多代理系统中信息共享不足和动态问题解决的需求。

Method: 引入黑板架构，代理根据黑板内容动态选择和执行，直至达成共识。

Result: 实验表明系统性能优于现有静态和动态多代理系统，且消耗更少token。

Conclusion: 该方法为复杂动态问题解决提供了新思路。

Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM
multi-agent systems (MASs) so that (1) agents with various roles can share all
the information and others' messages during the whole problem-solving process,
(2) agents that will take actions are selected based on the current content of
the blackboard, and (3) the selection and execution round is repeated until a
consensus is reached on the blackboard. We develop the first implementation of
this proposal and conduct experiments on commonsense knowledge, reasoning and
mathematical datasets. The results show that our system can be competitive with
the SOTA static and dynamic MASs by achieving the best average performance, and
at the same time manage to spend less tokens. Our proposal has the potential to
enable complex and dynamic problem-solving where well-defined structures or
workflows are unavailable.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [178] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Main category: cs.CE

TL;DR: 本文探讨了HPC与AI耦合的新方法，提出了三种耦合模式（替代、指导和协调），并通过材料科学案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决高性能计算（HPC）在科学应用中的高计算强度等挑战，探索HPC与AI（HPC-AI）耦合的潜力。

Method: 提出三种耦合模式：替代（surrogate）、指导（directive）和协调（coordinate），并通过材料科学案例研究验证。

Result: 展示了HPC-AI耦合在性能提升和技术挑战方面的效果，为科学发现提供了新视角。

Conclusion: 提出的耦合模式不仅适用于材料科学，还可推广到其他科学领域，为未来HPC-AI集成提供指导。

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [179] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: 论文提出‘Data Agent’概念，利用大语言模型（LLMs）提升数据系统的语义理解、推理和规划能力，以优化Data+AI应用的协调。


<details>
  <summary>Details</summary>
Motivation: 传统Data+AI系统依赖人工专家协调，缺乏语义理解和规划能力，LLMs的成功为改进提供了机会。

Method: 提出‘Data Agent’架构，整合知识理解、推理和规划能力，解决数据任务协调问题。

Result: 介绍了多种数据代理系统（如数据科学代理、数据分析代理等），并探讨了设计挑战。

Conclusion: Data Agent有望革新数据系统，但仍面临开放挑战。

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [180] [Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts](https://arxiv.org/abs/2507.01776)
*Yuxuan Yang*

Main category: cs.HC

TL;DR: 论文提出了一种人机协作框架，将机器学习的效率与人类创造力结合，以优化空间设计的情感与文化维度。


<details>
  <summary>Details</summary>
Motivation: 机器学习在空间设计中能提升效率，但无法单独处理情感、文化和美学需求，需要与人类创造力结合。

Method: 提出人机协作框架，机器学习自动化设计生成与优化，人类设计师负责情感和文化调整。

Result: 通过办公和住宅设计的案例研究，展示了框架如何平衡效率与情感共鸣。

Conclusion: 结合机器学习与人类创造力，空间设计可实现功能性与情感深度的平衡。

Abstract: The integration of machine learning (ML) into spatial design holds immense
potential for optimizing space utilization, enhancing functionality, and
streamlining design processes. ML can automate tasks, predict performance
outcomes, and tailor spaces to user preferences. However, the emotional,
cultural, and aesthetic dimensions of design remain crucial for creating spaces
that truly resonate with users-elements that ML alone cannot address. The key
challenge lies in harmonizing data-driven efficiency with the nuanced,
subjective aspects of design. This paper proposes a human-machine collaboration
framework to bridge this gap. An effective framework should recognize that
while ML enhances design efficiency through automation and prediction, it must
be paired with human creativity to ensure spaces are emotionally engaging and
culturally relevant. Human designers contribute intuition, empathy, and
cultural insight, guiding ML-generated solutions to align with users' emotional
and cultural needs. Additionally, we explore how various ML models can be
integrated with human-centered design principles. These models can automate
design generation and optimization, while human designers refine the outputs to
ensure emotional resonance and aesthetic appeal. Through case studies in office
and residential design, we illustrate how this framework fosters both
creativity and cultural relevance. By merging ML with human creativity, spatial
design can achieve a balance of efficiency and emotional impact, resulting in
environments that are both functional and deeply human.

</details>


### [181] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)
*Megan T. deBettencourt,Sruthi Sakthivel,Emily A. Holmes,Mark Chevillet*

Main category: cs.HC

TL;DR: ANTIDOTE结合AI和瞳孔测量技术，自动提供并监测基于证据的数字治疗（ICTI），显著减少创伤后侵入性记忆。


<details>
  <summary>Details</summary>
Motivation: 解决创伤治疗中人工指导的可扩展性问题，探索AI和神经技术作为替代方案。

Method: 100名健康志愿者观看创伤视频，随机分为干预组和对照组，AI指导结合瞳孔测量监测干预效果。

Result: 干预组报告侵入性记忆显著减少，瞳孔大小可作为干预效果的生物标志物。

Conclusion: AI指导的数字干预有望规模化应对创伤治疗需求。

Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can
help, but most require human guidance. Human guides provide tailored
instructions and responsiveness to internal cognitive states, but limit
scalability. Can generative AI and neurotechnology provide a scalable
alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to
automatically deliver and monitor an evidence-based digital treatment,
specifically the Imagery Competing Task Intervention (ICTI), to reduce
intrusive memories after psychological trauma. One hundred healthy volunteers
were exposed to videos of traumatic events and randomly assigned to an
intervention or active control condition. As predicted, intervention
participants reported significantly fewer intrusive memories over the following
week. Post-hoc assessment against clinical rubrics confirmed the AI guide
delivered the intervention successfully. Additionally, pupil size tracked
intervention engagement and predicted symptom reduction, providing a candidate
biomarker of intervention effectiveness. These findings open a path toward
rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [182] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)
*Vishakha Lall,Yisi Liu*

Main category: cs.HC

TL;DR: 本研究开发了一个AI驱动的框架，通过视觉焦点追踪、语音识别和压力检测，客观评估海事培训学员的表现，提升高风险场景的准备能力。


<details>
  <summary>Details</summary>
Motivation: 传统海事培训依赖主观评估，存在主观性、关键特征难以测量和认知限制等问题，AI框架旨在解决这些问题。

Method: 整合多种AI技术，包括视觉焦点追踪（眼动追踪、瞳孔分析、计算机视觉）、语音识别（海事专用语音转文本模型、自然语言处理）、沟通正确性（大型语言模型）和压力检测（声音音高分析）。

Result: AI算法在模拟海事场景中表现出高准确率：视觉检测约92%，语音识别约91%，压力检测约90%，超越现有基准。

Conclusion: 研究表明AI能通过提供客观性能分析、个性化反馈和提升实战准备能力，彻底改变海事培训。

Abstract: Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

</details>


### [183] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: 本文探讨了老年移民如何通过AI辅助共创表达个人叙事，结合汉字重构与口头叙述，无需数字素养即可将生活经验转化为视觉和触觉表达。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决老年移民群体叙事碎片化、代表性不足或难以言表的问题。

Method: 通过试点工作坊，结合口头叙述和汉字重构（使用小篆字形），利用大型语言模型（LLM）建议和实物材料，在人类引导和AI支持下完成共创。

Result: 参与者成功将生活经验转化为视觉和触觉表达，展示了AI作为支持机制而非内容生产者的潜力。

Conclusion: 该方法为人类-AI协作和老龄化研究提供了新视角，强调了AI在支持叙事能动性中的作用。

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


### [184] [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](https://arxiv.org/abs/2507.01719)
*Dorian Peters,Fernanda Espinoza,Marco da Re,Guido Ivetta,Luciana Benotti,Rafael A. Calvo*

Main category: cs.HC

TL;DR: 论文探讨了如何在多元文化和语言背景下开发适用于全球多数地区的健康对话AI（CAI），提出了一种基于拉丁美洲定性数据的自下而上方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLM）未能涵盖全球多样化的生活经验，需开发更文化敏感的CAI。

Method: 通过参与式工作坊收集拉丁美洲的定性数据，分析文化错位、区域观点及文化适应性策略。

Result: 研究发现文化概念需结合经济、政治、地理等多维度，提出‘多元宇宙健康对话AI’框架。

Conclusion: 需更多关系性和包容性，而不仅是数据量，来开发文化敏感的CAI。

Abstract: There is justifiable interest in leveraging conversational AI (CAI) for
health across the majority world, but to be effective, CAI must respond
appropriately within culturally and linguistically diverse contexts. Therefore,
we need ways to address the fact that current LLMs exclude many lived
experiences globally. Various advances are underway which focus on top-down
approaches and increasing training data. In this paper, we aim to complement
these with a bottom-up locally-grounded approach based on qualitative data
collected during participatory workshops in Latin America. Our goal is to
construct a rich and human-centred understanding of: a) potential areas of
cultural misalignment in digital health; b) regional perspectives on chatbots
for health and c)strategies for creating culturally-appropriate CAI; with a
focus on the understudied Latin American context. Our findings show that
academic boundaries on notions of culture lose meaning at the ground level and
technologies will need to engage with a broader framework; one that
encapsulates the way economics, politics, geography and local logistics are
entangled in cultural experience. To this end, we introduce a framework for
'Pluriversal Conversational AI for Health' which allows for the possibility
that more relationality and tolerance, rather than just more data, may be
called for.

</details>


### [185] [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](https://arxiv.org/abs/2507.01862)
*Sanjay Krishna Anbalagan,Xinrui Nie,Umesh Mohan,Vijay Kumar Kanamarlapudi,Anughna Kommalapati,Xiaodan Zhao*

Main category: cs.HC

TL;DR: 论文提出了一种将GUI中的“提交”和“重置”操作显式建模为LLM任务的方法，以提高领域特定聊天机器人的交互清晰度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统聊天机器人依赖语言线索管理上下文，易导致混淆和上下文不完整。受GUI启发，论文旨在通过显式任务建模解决这一问题。

Method: 将GUI中的“提交”和“重置”操作建模为LLM的显式任务，并利用结构化会话数据捕获用户确认、重置及推理链。

Result: 在酒店预订和客户管理场景中，该方法提升了多轮任务连贯性、用户满意度和效率。

Conclusion: 显式任务建模能有效改善聊天机器人的上下文管理和用户体验。

Abstract: Domain specific chatbot applications often involve multi step interactions,
such as refining search filters, selecting multiple items, or performing
comparisons. Traditional graphical user interfaces (GUIs) handle these
workflows by providing explicit "Submit" (commit data) and "Reset" (discard
data) actions, allowing back-end systems to track user intent unambiguously. In
contrast, conversational agents rely on subtle language cues, which can lead to
confusion and incomplete context management. This paper proposes modeling these
GUI inspired metaphors acknowledgment (submit like) and context switching
(reset-like) as explicit tasks within large language model (LLM) prompts. By
capturing user acknowledgment, reset actions, and chain of thought (CoT)
reasoning as structured session data, we preserve clarity, reduce user
confusion, and align domain-specific chatbot interactions with back-end logic.
We demonstrate our approach in hotel booking and customer management scenarios,
highlighting improvements in multi-turn task coherence, user satisfaction, and
efficiency.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [186] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: 研究开发了一种可解释的深度学习框架，通过海马功能连接预测大脑年龄，揭示了与年龄相关的关键海马-皮层连接。


<details>
  <summary>Details</summary>
Motivation: 海马灰质减少是神经生物衰老的标志，但其功能连接的变化尚不清楚，研究旨在填补这一空白。

Method: 使用三维卷积神经网络（3D CNN）结合LayerCAM显著性映射，分析海马功能连接。

Result: 发现与年龄高度敏感的海马-皮层连接区域，并区分了前后海马功能连接的不同模式。

Conclusion: 研究为海马衰老的功能机制提供了新见解，展示了可解释深度学习在神经影像数据中的潜力。

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [187] [Can AI be Consentful?](https://arxiv.org/abs/2507.01051)
*Giada Pistilli,Bruna Trevelin*

Main category: cs.CY

TL;DR: 传统法律和伦理框架围绕同意的概念在生成式AI系统中面临挑战，无法有效应对AI生成内容的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 探讨传统同意概念在AI生成内容中的不足，揭示其在数据保护和个人隐私方面的局限性。

Method: 通过法律和伦理分析，识别出三个核心挑战：范围问题、时间性问题、自主陷阱，并提出了“同意鸿沟”的概念。

Result: 当前法律框架未能充分应对AI生成内容带来的新挑战，特别是在个人自主权、身份权利和社会责任方面。

Conclusion: 需要发展新的伦理和法律方法来解决AI系统中的同意问题，以更好地保护个人权利和社会责任。

Abstract: The evolution of generative AI systems exposes the challenges of traditional
legal and ethical frameworks built around consent. This chapter examines how
the conventional notion of consent, while fundamental to data protection and
privacy rights, proves insufficient in addressing the implications of
AI-generated content derived from personal data. Through legal and ethical
analysis, we show that while individuals can consent to the initial use of
their data for AI training, they cannot meaningfully consent to the numerous
potential outputs their data might enable or the extent to which the output is
used or distributed. We identify three fundamental challenges: the scope
problem, the temporality problem, and the autonomy trap, which collectively
create what we term a ''consent gap'' in AI systems and their surrounding
ecosystem. We argue that current legal frameworks inadequately address these
emerging challenges, particularly regarding individual autonomy, identity
rights, and social responsibility, especially in cases where AI-generated
content creates new forms of personal representation beyond the scope of the
original consent. By examining how these consent limitations intersect with
broader principles of responsible AI (including fairness, transparency,
accountability, and autonomy) we demonstrate the need to evolve ethical and
legal approaches to consent.

</details>


### [188] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Main category: cs.CY

TL;DR: Epitome是一个开创性的开放实验平台，将大型语言模型（LLMs）与社会科学研究深度融合，旨在研究AI对个体、组织和社会的交互影响。


<details>
  <summary>Details</summary>
Motivation: 探索AI在现实世界部署中对社会各层面的影响，促进跨学科研究。

Method: 通过七个核心模块提供一站式实验解决方案，嵌入经典社会科学实验逻辑，支持多级人机交互环境。

Result: 成功复现三项经典社会科学实验，验证平台在复杂实验设计和结果稳健性方面的潜力。

Conclusion: Epitome为AI与社会科学交叉研究提供了高效工具，对政策制定等领域具有潜在应用价值。

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>


### [189] [Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review](https://arxiv.org/abs/2507.01062)
*Seyma Yaman Kayadibi*

Main category: cs.CY

TL;DR: 论文探讨了生成式AI（如ChatGPT）在高等教育中的应用，结合文献综述和模拟建模分析学生对其的看法及其对学习成果的影响。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在高等教育中的学生认知及其对学习成果的潜在影响。

Method: 采用混合方法，包括系统性文献综述和基于模拟的建模，分析19篇实证文章。

Result: 发现学生对生成式AI的实用性和现实有用性的态度是学习成果的显著预测因素。

Conclusion: 研究为生成式AI在高等教育中的合理使用提供了跨学科视角。

Abstract: The exponential development of generative artificial intelligence (GenAI)
technologies like ChatGPT has raised increasing curiosity about their use in
higher education, specifically with respect to how students view them, make use
of them, and the implications for learning outcomes. This paper employs a
hybrid methodological approach involving a systematic literature review and
simulation-based modeling to explore student perceptions of GenAI use in the
context of higher education. A total of nineteen empirical articles from 2023
through 2025 were selected from the PRISMA-based search targeting the Scopus
database. Synthesis of emerging patterns from the literature was achieved by
thematic categorization. Six of these had enough quantitative information,
i.e., item-level means and standard deviations, to permit probabilistic
modeling. One dataset, from the resulting subset, was itself selected as a
representative case with which to illustrate inverse-variance weighting by
Monte Carlo simulation, by virtue of its well-designed Likert scale format and
thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength
of the relationship between student perceptions and learning achievements.
Findings reveal that attitude factors concerned with usability and real-world
usefulness are significantly better predictors of positive learning achievement
than affective or trust-based factors. Such an interdisciplinary perspective
provides a unique means of linking thematic results with predictive modelling,
resonating with longstanding controversies about the proper use of GenAI tools
within the university.

</details>


### [190] [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](https://arxiv.org/abs/2507.01418)
*Inyoung Cheong,Alicia Guo,Mina Lee,Zhehui Liao,Kowe Kadoma,Dongyoung Go,Joseph Chee Chang,Peter Henderson,Mor Naaman,Amy X. Zhang*

Main category: cs.CY

TL;DR: 研究探讨AI使用声明对写作质量评价的影响，发现人类和LLM评分者均对AI使用持负面态度，但LLM评分者还表现出对特定身份作者的偏好，且这种偏好在AI披露后消失。


<details>
  <summary>Details</summary>
Motivation: 随着AI在写作中的普及，透明性要求增加，但不同身份群体可能因透明性承担不同代价。研究旨在揭示AI披露声明如何影响写作评价，以及这种影响是否因作者身份而异。

Method: 通过大规模对照实验，人类评分者（1,970人）和LLM评分者（2,520人）评估同一篇新闻文章，同时系统变化披露声明和作者人口统计信息。

Result: 人类和LLM评分者均对AI使用持负面评价。LLM评分者还表现出对女性或黑人作者的偏好，但这种偏好在AI披露后消失。

Conclusion: 研究揭示了AI披露与作者身份之间的复杂关系，凸显了机器与人类评价模式的差异。

Abstract: As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

</details>


### [191] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Main category: cs.CY

TL;DR: 本文探讨了人工智能（AI）如何通过数字技术提升交通基础设施的损害评估与监测能力，特别关注桥梁损害检测的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 交通基础设施对经济增长至关重要，但面临老化、气候变化和混合威胁（如自然灾害和网络攻击）的风险，需要更有效的监测和评估方法。

Method: 通过系统性文献综述，分析了现有AI模型和数据集在道路、桥梁等基础设施损害评估中的应用，特别关注SAR数据与AI模型的结合。

Result: 研究发现，目前缺乏将AI模型应用于SAR数据进行桥梁全面损害评估的研究，存在显著的研究空白。

Conclusion: 本文旨在填补研究空白，为AI驱动的交通基础设施评估与监测解决方案奠定基础。

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [192] [Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance](https://arxiv.org/abs/2507.01638)
*Ana Nikolikj,Gabriela Ochoa,Tome Eftimov*

Main category: cs.NE

TL;DR: 分析多目标组合优化算法性能的景观特征，基于C-PLOS-net模型，研究不同算法在rmnk-landscapes上的表现。


<details>
  <summary>Details</summary>
Motivation: 探索景观特征如何影响多目标组合优化算法的性能，为特定算法和景观提供定制化分析。

Method: 使用C-PLOS-net模型提取景观特征，测试PLS、GSEMO和NSGA-II算法在rmnk-landscapes上的表现，评估分辨率和超体积指标。

Result: 发现特定景观特征组合对算法性能有显著影响，揭示了特征重要性。

Conclusion: 研究为特定算法和景观提供了深入的性能分析工具，有助于优化算法选择。

Abstract: We present an analysis of landscape features for predicting the performance
of multi-objective combinatorial optimization algorithms. We consider features
from the recently proposed compressed Pareto Local Optimal Solutions Networks
(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a
set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness
and objective correlation. We consider the performance of three algorithms --
Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and
Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and
hypervolume metrics. Our tailored analysis reveals feature combinations that
influence algorithm performance specific to certain landscapes. This study
provides deeper insights into feature importance, tailored to specific
rmnk-landscapes and algorithms.

</details>


### [193] [Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis](https://arxiv.org/abs/2507.01668)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: 论文探讨了如何通过统计测试比较元启发式算法的搜索行为，以区分其创新性。


<details>
  <summary>Details</summary>
Motivation: 针对当前元启发式算法领域因隐喻命名导致创新性模糊的问题，研究旨在通过统计方法区分算法的实际差异。

Method: 使用交叉匹配统计测试比较多元分布，评估MEALPY库中114种算法的解。

Result: 通过实证分析识别出具有相似搜索行为的算法。

Conclusion: 统计测试为区分算法创新性提供了有效工具。

Abstract: The field of numerical optimization has recently seen a surge in the
development of "novel" metaheuristic algorithms, inspired by metaphors derived
from natural or human-made processes, which have been widely criticized for
obscuring meaningful innovations and failing to distinguish themselves from
existing approaches. Aiming to address these concerns, we investigate the
applicability of statistical tests for comparing algorithms based on their
search behavior. We utilize the cross-match statistical test to compare
multivariate distributions and assess the solutions produced by 114 algorithms
from the MEALPY library. These findings are incorporated into an empirical
analysis aiming to identify algorithms with similar search behaviors.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [194] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Main category: cs.RO

TL;DR: 提出了一种结合采样和搜索的运动规划算法，利用自由配置空间的burs作为自适应运动基元，显著提高了路径规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统固定大小运动基元在复杂场景中效率低下，需要更高效的自适应方法。

Method: 使用自由配置空间的burs作为自适应运动基元，嵌入图搜索算法中，实现高效探索。

Result: 在复杂场景中，尤其是高自由度机械臂，性能优于固定基元方法，简单场景中表现相当。

Conclusion: burs作为自适应运动基元能显著提升运动规划效率，适用于复杂和高自由度场景。

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [195] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Main category: cs.RO

TL;DR: 提出了一种利用大语言模型（LLM）生成驾驶场景代码的框架，结合CARLA模拟器和视频生成技术，用于自动驾驶系统的安全测试。


<details>
  <summary>Details</summary>
Motivation: 设计多样且安全的驾驶场景对评估自动驾驶系统至关重要，但传统方法难以高效生成复杂场景。

Method: 通过LLM基于少量示例生成场景脚本，结合CARLA模拟器和视频生成技术（Cosmos-Transfer1与ControlNet）提升场景真实感。

Result: 实验表明，该方法能生成多样、真实且安全关键的驾驶场景，尤其是罕见边缘案例。

Conclusion: 该框架为自动驾驶模拟测试提供了高效且可控的工具。

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [196] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: 提出了VLAD模型，结合视觉语言模型（VLM）与先进自动驾驶系统（VAD），通过定制问答数据集微调提升空间推理能力，减少碰撞率31.82%。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型（如LLaVA、Qwen-VL）的通用知识增强自动驾驶的感知、预测和规划能力。

Method: 提出VLAD模型，结合微调VLM与VAD系统，采用定制问答数据集提升空间推理，生成导航命令和可解释的自然语言决策。

Result: 在nuScenes数据集上测试，碰撞率平均降低31.82%，为VLM增强自动驾驶系统设定了新标准。

Conclusion: VLAD模型通过结合VLM与VAD，显著提升自动驾驶性能，并增加系统透明度和可信度。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [197] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: 研究探讨了混合量子-经典算法在工业环境中优化机器人检测轨迹的应用，与传统方法相比，量子方法在计算时间和解质量上表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在工业自动化（如机器人检测轨迹优化）中的潜力，以提升效率并适应工业4.0需求。

Method: 将任务建模为3D旅行商问题（TSP），使用D-Wave量子求解器与经典方法（GUROBI、OR-Tools）对比。

Result: 在五个实际案例中，量子方法在计算时间显著减少的同时，解质量与传统方法相当。

Conclusion: 量子方法在工业自动化中具有潜力，尤其在计算效率方面表现突出。

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [198] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Main category: cs.RO

TL;DR: BioMARS是一个结合LLMs、VLMs和模块化机器人的智能平台，用于自主设计和执行生物实验，性能优于人工操作。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLMs和VLMs在生物研究应用中存在的协议设计僵化、适应性不足、错误处理差和操作复杂的问题。

Method: 采用分层架构：Biologist Agent生成协议，Technician Agent转化为可执行代码，Inspector Agent通过多模态感知确保程序完整性。

Result: 在细胞传代和培养任务中性能优于人工，并在视网膜色素上皮细胞分化中优于传统策略。

Conclusion: BioMARS展示了通用AI驱动实验室自动化的可行性，凸显语言推理在生物研究中的变革作用。

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


### [199] [LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction](https://arxiv.org/abs/2507.01308)
*Muhammad Atta ur Rahman,Dooseop Choi,KyoungWook Min*

Main category: cs.RO

TL;DR: 提出了一种基于多向量地图元素的运动预测模型，通过融合车道边界和道路边缘等信息，提升自动驾驶车辆在复杂交通场景中的轨迹预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于车道中心线的运动预测模型无法充分捕捉道路环境和交通规则，限制了预测的准确性。

Method: 开发了多向量地图元素的特征融合策略和有效的连接剪枝机制，以减少计算开销并保持关键空间和语义关系。

Result: 在Argoverse 2数据集上验证了方法的有效性，性能优于现有基于车道中心线的模型。

Conclusion: 该方法提供了更丰富、高效的驾驶环境表示，推动了自动驾驶运动预测的先进水平。

Abstract: Accurate motion forecasting is critical for safe and efficient autonomous
driving, enabling vehicles to predict future trajectories and make informed
decisions in complex traffic scenarios. Most of the current designs of motion
prediction models are based on the major representation of lane centerlines,
which limits their capability to capture critical road environments and traffic
rules and constraints. In this work, we propose an enhanced motion forecasting
model informed by multiple vector map elements, including lane boundaries and
road edges, that facilitates a richer and more complete representation of
driving environments. An effective feature fusion strategy is developed to
merge information in different vector map components, where the model learns
holistic information on road structures and their interactions with agents.
Since encoding more information about the road environment increases memory
usage and is computationally expensive, we developed an effective pruning
mechanism that filters the most relevant map connections to the target agent,
ensuring computational efficiency while maintaining essential spatial and
semantic relationships for accurate trajectory prediction. Overcoming the
limitations of lane centerline-based models, our method provides a more
informative and efficient representation of the driving environment and
advances the state of the art for autonomous vehicle motion forecasting. We
verify our approach with extensive experiments on the Argoverse 2 motion
forecasting dataset, where our method maintains competitiveness on AV2 while
achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements,
road topology, connection pruning, Argoverse 2.

</details>


### [200] [AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation](https://arxiv.org/abs/2507.01961)
*Sixiang Chen,Jiaming Liu,Siyuan Qian,Han Jiang,Lily Li,Renrui Zhang,Zhuoyang Liu,Chenyang Gu,Chengkai Hou,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 论文提出AC-DiT模型，通过协调移动底座和机械臂的动作，并动态调整视觉模态的融合权重，以提升移动操作任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在协调移动底座和机械臂时存在不足，且未能动态适应不同阶段的感知需求。

Method: 提出AC-DiT模型，包含移动底座到机械臂的条件机制和感知感知的多模态融合策略。

Result: 实验验证了AC-DiT在模拟和真实移动操作任务中的有效性。

Conclusion: AC-DiT通过协调和动态感知优化了移动操作任务的表现。

Abstract: Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [201] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: 论文提出PathCoT方法，通过整合病理学专家知识和自评估机制，提升多模态大语言模型在病理视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在病理视觉推理任务中表现不佳，主要因缺乏领域知识和额外推理步骤引入错误。

Method: 提出PathCoT，结合病理专家知识和自评估步骤，优化零样本CoT提示方法。

Result: 在PathMMU数据集上验证了PathCoT的有效性。

Conclusion: PathCoT通过专家知识和自评估显著提升了病理视觉推理的准确性。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [202] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的多视图动态决策框架，用于多组学数据分类，旨在降低测试成本的同时保持高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 高通量多组学技术成本高昂，可能导致资源浪费，因此需要一种方法在保证诊断准确性的同时减少冗余测试。

Method: 在单组学层面，通过改进神经网络激活函数生成Dirichlet分布参数，量化分类结果的置信度和不确定性；在多组学层面，基于Dempster-Shafer理论融合异构模态，动态引入数据直至满足置信度阈值。

Result: 在四个多组学数据集上验证，50%以上病例仅需单组学数据即可准确分类，同时保持与全组学模型相当的诊断性能。

Conclusion: 该方法有效减少了冗余测试，同时保持了诊断准确性和生物学见解。

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [203] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: 该研究提出了一种数据驱动的方法，利用历史数据预测2025年利比亚班加西的电力负荷、发电量和缺口，LSTM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 准确的电力预测对电网稳定和能源规划至关重要，尤其是在班加西这种电力供应不稳定的地区。

Method: 使用多种时间序列模型（如ARIMA、季节性ARIMA、LSTM等），并对数据进行缺失值填补、异常值平滑和对数转换。

Result: LSTM模型在预测非平稳和季节性模式方面表现最优，且整合了温度和湿度等外生因素。

Conclusion: 研究结果为政策制定者和电网运营商提供了实用见解，有助于在数据稀缺且不稳定的地区进行主动负荷管理和资源规划。

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [204] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: 该研究通过结合GNN和LLM的混合推荐系统，优化推理延迟和训练效率，采用量化、LoRA、蒸馏等方法，结合硬件加速（FPGA、DeepSpeed），显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在线服务对高效推荐系统的需求日益增长，需要解决混合GNN和LLM推荐系统的计算瓶颈问题。

Method: 采用混合GNN-LLM架构，结合量化、LoRA、蒸馏等优化策略，并使用FPGA和DeepSpeed进行硬件加速。

Result: 最优配置（Hybrid + FPGA + DeepSpeed）在NDCG@10上达到0.75，延迟40-60ms；LoRA将训练时间减少66%（3.8小时）。

Conclusion: 硬件-软件协同设计和参数高效调优使混合模型优于独立GNN或LLM方法，推荐使用FPGA和LoRA进行实时部署。未来可探索联邦学习和高级融合架构。

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [205] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FSTA的分解技术，结合L2Seg神经网络框架，显著加速了车辆路径问题（VRP）的迭代求解器。


<details>
  <summary>Details</summary>
Motivation: 现有迭代求解器在解决大规模VRP时存在冗余计算问题，因为大部分解在搜索迭代中保持稳定。

Method: FSTA技术保留稳定解段，将节点聚合为超节点，专注于不稳定部分；L2Seg通过神经网络智能区分稳定与不稳定部分。

Result: 实验表明，L2Seg将最先进求解器的速度提升高达7倍，且NAR与AR的协同效果最佳。

Conclusion: L2Seg是一个灵活框架，适用于传统、基于学习和混合求解器，支持广泛的VRP问题。

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [206] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: 提出了一种基于PPO的强化学习方法训练神经模糊控制器，相比之前的DQN方法，表现更稳定且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 改进现有的基于DQN的神经模糊控制器训练方法，利用PPO的稳定性提升性能。

Method: 使用PPO替代DQN，构建稳定的on-policy actor-critic循环，并在CartPole-v1环境中评估。

Result: PPO训练的模糊控制器在CartPole-v1中平均回报达到500 +/- 0，方差更小且收敛更快。

Conclusion: PPO为训练可解释的神经模糊控制器提供了一种有前景的方法。

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [207] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: Clifford Neural Layers通过引入Clifford代数优化PDE建模，在CPU上实现比标准PyTorch快30%的性能。


<details>
  <summary>Details</summary>
Motivation: 提升2/3D Clifford卷积层和多向量激活层在单核CPU上的推理效率。

Method: 优化Clifford卷积层和多向量激活层的实现，测试其性能。

Result: 在较大数据和网络规模下，性能比标准PyTorch实现快30%。

Conclusion: 开源代码库，展示了Clifford Neural Layers在PDE建模中的高效性。

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [208] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: 提出了一种基于DAG的快速模型分割算法，通过最大流方法实现最优分割，显著降低训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决复杂AI模型分割的高计算复杂度问题，提升设备端计算效率。

Method: 将AI模型表示为DAG，转化为最小s-t割问题，提出快速DAG分割算法和块状分割算法。

Result: 算法在毫秒级找到最优分割，动态边缘网络中训练延迟降低24.62%-38.95%。

Conclusion: 提出的算法高效且最优，适用于复杂AI模型分割，显著提升计算效率。

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [209] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: 提出了一种动态调整神经网络架构的新方法，通过蒙特卡洛树搜索实现训练过程中的架构优化，适用于多变量时间序列分类。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络架构固定，无法动态调整，限制了模型性能。本文旨在通过动态调整架构提升模型表现。

Method: 使用蒙特卡洛树搜索模拟网络行为，动态调整神经网络架构（扩展或收缩），并在训练过程中选择最优架构。

Result: 在多变量时间序列分类任务中表现优异，验证了方法的鲁棒性和适应性。

Conclusion: 动态调整架构的方法显著提升了模型性能，尤其在多变量时间序列分类中效果突出。

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [210] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer架构的心脏感知基础模型（CSFM），通过生成式掩码预训练策略从多模态数据中学习统一表示，显著提升心脏信号分析的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法依赖同质数据集和静态模型，限制了其在不同临床环境和采集协议中的泛化能力。CSFM旨在解决这一问题。

Method: 利用Transformer架构和生成式掩码预训练策略，整合多模态数据（包括MIMIC-III-WDB、MIMIC-IV-ECG和CODE数据集），覆盖约170万人的心脏信号和文本报告。

Result: CSFM在诊断任务、人口统计信息识别、生命体征测量、临床结果预测和ECG问答中表现优于传统单模态单任务方法，支持多种ECG导联配置和PPG信号。

Conclusion: CSFM是一种多功能、可扩展的心脏监测解决方案，具有广泛的应用潜力。

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [211] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: 提出了一种两阶段训练框架，用于高效去毒文本，同时保持语义和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上毒害内容的广泛传播威胁在线环境和公共讨论，现有方法在去毒性能、语义保持和数据效率上存在不足。

Method: 两阶段训练：先用高质量平行数据微调，再用无标签数据和奖励模型通过Group Relative Policy Optimization训练LLM。

Result: 实验表明该方法有效平衡去毒、语义保持和泛化，减少对标注数据的依赖，达到最优性能。

Conclusion: 该方法显著提升了去毒效果和泛化能力，同时降低了对标注数据的需求。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [212] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: 提出了一种新型能量函数，用于长序列记忆，基于密集Hopfield网络框架，通过高阶交互实现指数存储容量。引入时间核以整合时间依赖性，成功应用于电影帧的存储与顺序检索。


<details>
  <summary>Details</summary>
Motivation: 解决长序列任务中Transformer的局限性，如长上下文建模、时间序列数据的长期依赖处理。

Method: 基于密集Hopfield网络，提出时间核$K(m, k)$以整合时间依赖性，实现高效顺序检索。

Result: 成功应用于电影帧的存储与顺序检索，展示了在高维向量空间中的有效性。

Conclusion: 该模型为长序列任务提供了有前景的解决方案，对自然语言处理、预测等领域有潜在影响。

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [213] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: 提出了一种基于元素组成和XRD的多模态框架，无需晶体结构输入，通过自监督预训练提升材料发现的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有结构模型依赖晶体结构数据的问题，适用于实验数据中结构未知或难以获取的情况。

Method: 采用多模态框架，结合元素组成和XRD数据，使用自监督预训练策略（MXM和对比对齐）。

Result: 预训练显著加速收敛（最高4.2倍），提升准确性和表示质量，多模态性能随数据规模增长更优。

Conclusion: 为材料科学提供了一种无需结构输入、基于实验数据的基础模型路径。

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [214] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: 本文优化了一种先进的基模型，用于预测高性能机器学习服务中的罕见、尖峰事件，并与经典随机模型进行了比较。


<details>
  <summary>Details</summary>
Motivation: 预测罕见、尖峰事件（如生产中断）是极具挑战性的任务，目前基模型尚未在此领域应用。

Method: 优化基模型并与经典随机模型（如移动平均和自回归模型）进行比较，分析其预测误差。

Result: 基模型在预测尖峰事件时表现优于随机模型，并能以小于6%的误差估计年度中断统计数据。

Conclusion: 基模型在罕见事件预测中具有潜力，为实际应用提供了更准确的工具。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [215] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 论文评估了大型脑波基础模型（LBMs）在脑机接口（BCI）任务中的表现，发现其性能提升有限且参数需求高，提出了通过LoRA技术优化参数效率的方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型脑波基础模型（LBMs）在脑波建模中的潜力，评估其在BCI任务中的适用性和效率。

Method: 通过系统微调实验和低秩适应（LoRA）技术，比较LBMs与传统深度架构的性能和参数效率。

Result: LBMs性能提升有限（0.9%-1.2%），但参数需求显著增加；LoRA能减少参数且不降低性能。

Conclusion: LBMs需要领域特定的优化策略，当前架构可能需要重新设计以充分发挥基础模型在脑波分析中的潜力。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [216] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型（LLM）训练的随机共轭次梯度方法，结合自适应采样，相比传统SGD方法，实现了更快的收敛速度和更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度下降（SGD）在大规模应用中表现出性能限制，需要更高效的优化方法。

Method: 提出了一种随机共轭次梯度方法，结合自适应采样和AdamW-like算法调整步长，以解决LLM训练中的非凸性和非光滑性问题。

Result: 实验结果表明，该方法不仅保持了传统SGD的可扩展性，还在速度和准确性上显著提升。

Conclusion: 该方法为LLM训练提供了一种更高效的优化方案，兼具快速收敛和可扩展性。

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [217] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: 本文提出了一种名为PULSE的协议，用于评估大型多模态模型（LMMs）中的遗忘技术，重点关注预训练知识遗忘和长期可持续性评估。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘基准仅关注单次遗忘操作，缺乏对预训练知识遗忘和多次遗忘请求的评估。

Method: 引入PULSE协议，从预训练知识遗忘和长期可持续性两个维度评估现有遗忘方法。

Result: 研究发现，现有方法能有效遗忘微调知识，但难以消除预训练信息；且单次批量遗忘方法在多次操作中性能显著下降。

Conclusion: PULSE协议为LMMs的遗忘技术提供了更全面的评估框架，揭示了当前方法的局限性。

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [218] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的神经哈密顿算子（NHO）框架，用于解决高维随机控制问题，通过神经网络参数化FBSDE系统，并证明其通用逼近能力。


<details>
  <summary>Details</summary>
Motivation: 高维随机控制问题因维度灾难难以解决，传统动态规划方法效果有限，因此需要新的方法。

Method: 引入神经哈密顿算子（NHO），用神经网络参数化FBSDE系统，并通过训练网络满足PMP一致性条件。

Result: 证明了NHO在一般鞅驱动下的通用逼近能力，并分析了优化挑战。

Conclusion: NHO框架为高维随机控制问题提供了新的深度学习解决方案，具有理论支持和实际潜力。

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [219] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO是一种自引导过程奖励优化框架，通过内在推导过程奖励和引入累积过程奖励与掩码步骤优势，显著提升了训练效率和测试准确性，同时保持计算开销不变。


<details>
  <summary>Details</summary>
Motivation: 解决过程强化学习中计算开销大和缺乏统一理论框架的问题。

Method: 提出SPRO框架，包括内在推导过程奖励和引入累积过程奖励与掩码步骤优势（MSA）。

Result: SPRO训练效率提升3.4倍，测试准确性提高17.5%，响应长度减少约1/3。

Conclusion: SPRO在保持计算效率的同时，显著提升了过程强化学习的性能，适用于工业实现。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [220] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出双学习假说，揭示LLMs在上下文学习中同时学习任务相关和潜在后门概念，并提出ICLShield防御机制，动态调整概念偏好比例，显著提升防御效果。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）因其适应性和无参数特性在大型语言模型（LLMs）中表现优异，但也易受后门攻击。本文旨在解决这一安全漏洞。

Method: 提出双学习假说，理论分析ICL后门效应上限，并设计ICLShield防御机制，通过动态调整概念偏好比例和利用置信度与相似性分数选择干净示例。

Result: 实验表明，ICLShield在多种LLMs和任务中实现最先进的防御效果，平均提升26.02%，且在闭源模型（如GPT-4）中表现优异。

Conclusion: 双学习假说为理解ICL后门攻击提供了新视角，ICLShield有效提升了防御能力，具有广泛适用性。

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [221] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出了一种基于自适应困惑度感知强化学习（APARL）的框架，用于客户服务对话中的异常事件检测，显著提升了模型的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现实客户服务对话中异常事件检测的复杂性及模型跨领域（OOD）泛化能力不足的问题。

Method: 采用双环动态课程学习架构，利用大语言模型的推理能力，逐步聚焦更具挑战性的样本。

Result: 在食品配送对话任务中，F1分数平均提升17.19%，OOD迁移测试平均提升9.59%。

Conclusion: APARL为工业部署提供了高效的异常检测解决方案，提升了运营效率和商业价值。

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [222] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: 论文提出了一种名为Prefix-RFT的混合方法，结合了监督微调（SFT）和强化微调（RFT）的优势，在数学推理任务中表现出色，且易于集成到现有框架中。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型后训练技术（SFT和RFT）各有优缺点，SFT易过拟合，RFT性能不稳定。论文旨在提出一种统一方法，结合两者的优势。

Method: 提出Prefix-RFT，一种混合方法，同时利用示范数据和探索学习，通过数学推理任务验证其有效性。

Result: Prefix-RFT性能优于单独的SFT和RFT，且优于并行混合策略RFT方法，对示范数据的质量和数量变化具有鲁棒性。

Conclusion: Prefix-RFT成功结合了SFT和RFT的优势，为未来研究提供了统一范式的新视角。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [223] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER是一种基于黑盒优化的进化方法，用于大型语言模型（LLM）的后训练，通过隐式压缩训练数据引入信息瓶颈，提供理论保证，并在隐私敏感环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 梯度优化依赖大量标注数据，存在隐私和安全问题；黑盒优化在数据受限或对抗风险高时表现更好，但面临可扩展性和计算成本挑战。

Method: 提出BBoxER方法，通过信息瓶颈隐式压缩数据，提供理论保证，适用于预训练LLM的轻量级增强。

Result: 实验表明，BBoxER在推理数据集上提升性能并泛化良好，适合作为梯度优化的补充。

Conclusion: BBoxER是一种高效的黑盒优化方法，适用于隐私敏感环境，并具有理论保障。

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [224] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: 提出了一种基于CPU的低秩适配器（LoRA）微调方法，适用于计算资源有限的用户，通过组合预训练适配器生成新适配器，性能优于基础模型但不及GPU训练版本。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA微调对GPU的依赖问题，为计算资源有限的用户提供可行方案。

Method: 利用预训练适配器库，通过轻量级组合生成新适配器，避免梯度更新，直接在CPU上完成。

Result: 生成的适配器性能优于基础Mistral模型，但不及GPU训练的适配器。

Conclusion: 该方法为资源有限的用户提供了实用的LoRA微调替代方案。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [225] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: DSAC-D算法通过扩散模型和多模态分布解决了传统强化学习中值函数估计的偏差问题，并在控制任务中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用单模态分布（如高斯分布）建模值分布，容易导致估计偏差和算法性能下降。

Method: 提出DSAC-D算法，结合扩散模型和多模态分布，构建扩散值网络和策略网络，实现双扩散。

Result: 在MuJoCo任务中表现优异，抑制了估计偏差，平均回报提升10%以上；真实车辆测试中能准确表征多模态驾驶风格。

Conclusion: DSAC-D在多模态策略学习和性能提升方面具有显著优势。

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [226] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1是一种反射生成模型，通过自监督过程奖励模型（SPRM）实现高效推理，参数减少99%，性能接近OpenAI o3。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一接口整合策略模型和过程奖励模型，减少额外标注需求，提升推理效率。

Method: 共享主干网络，使用任务特定头进行下一令牌预测和过程评分，结合SPRM实现高效推理。

Result: MetaStone-S1在32B参数规模下性能接近OpenAI-o3-mini系列，并支持可扩展的推理模式。

Conclusion: MetaStone-S1展示了高效推理的潜力，并通过开源推动研究社区发展。

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [227] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: 本文提出了一种基于TVM编译器的工作流，用于高效地将AI工作负载映射到RISC-V向量单元，相比现有方法提升了性能并减少了代码内存占用。


<details>
  <summary>Details</summary>
Motivation: RISC-V向量扩展（RVV）在AI工作负载加速中具有潜力，但缺乏高效的自动调优框架，限制了其实际部署。

Method: 将RVV扩展集成到TVM的MetaSchedule框架中，通过概率程序框架调优张量操作，并在FPGA上实现多种RISC-V SoC进行验证。

Result: 相比GCC的自动向量化和muRISCV-NN，执行延迟分别平均提升46%和29%，且代码内存占用更小。在商用RISC-V SoC上，性能平均提升35%。

Conclusion: 该方法显著提升了RISC-V向量单元的利用效率，适用于嵌入式设备，并已开源以支持社区扩展。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [228] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: 论文重新审视了强化学习中奖励频率作为任务难度衡量指标的假设，指出当前方法在关键子目标无直接奖励时的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨奖励频率假设的可靠性，揭示当前策略学习方法在零激励动态（关键子目标无奖励）中的不足。

Method: 形式化零激励动态问题，分析现有深度子目标算法在此类动态中的表现。

Result: 发现学习性能对子目标完成与最终奖励的时间接近性高度敏感，现有方法无法有效利用零激励动态。

Conclusion: 指出当前方法的基本限制，强调需要能推断潜在任务结构而不依赖即时奖励的机制。

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [229] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: Chargax是一个基于JAX的电动汽车充电站仿真环境，用于加速强化学习训练，性能提升100x-1000x，支持多样化配置。


<details>
  <summary>Details</summary>
Motivation: 电网系统拥堵问题亟需提升运营效率，传统强化学习方法因样本复杂性和仿真成本高而效率低下。

Method: 开发了JAX-based的Chargax环境，支持真实数据场景下的仿真和RL代理训练。

Result: Chargax性能提升100x-1000x，并能模拟多样化的真实充电站配置。

Conclusion: Chargax为可持续能源挑战提供了高效的强化学习解决方案。

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [230] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: 论文提出了一种名为GradMetaNet的新架构，专门设计用于处理神经网络梯度，基于三个原则：等变性设计、多数据点梯度处理和高效梯度表示。


<details>
  <summary>Details</summary>
Motivation: 当前处理梯度的学习算法未针对梯度处理专门设计，限制了其适用性。

Method: 基于等变性设计、多数据点梯度处理和高效梯度表示（秩1分解）构建GradMetaNet架构。

Result: GradMetaNet能够逼近自然梯度函数，优于现有方法，并在多种任务（如优化和编辑）中表现优异。

Conclusion: GradMetaNet为梯度处理提供了高效且通用的解决方案，适用于多种任务。

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [231] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: AsyncFlow是一个异步流式强化学习框架，用于高效后训练，解决了现有框架的可扩展性和资源利用问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架在大型语言模型后训练中存在可扩展性瓶颈和资源闲置问题，AsyncFlow旨在解决这些问题。

Method: 提出分布式数据存储和传输模块，支持流式数据管理和细粒度调度；设计基于生产者-消费者的异步工作流，延迟参数更新以减少计算闲置。

Result: 实验显示吞吐量平均提升1.59倍。

Conclusion: AsyncFlow为下一代强化学习训练系统设计提供了模块化和可定制的解决方案。

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [232] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: 论文提出了一种联合自动编码调制器（JAM）框架，用于优化视觉和语言模型之间的表示对齐，通过多目标优化任务实现模态间的共享结构。


<details>
  <summary>Details</summary>
Motivation: 探索视觉和语言模型在独立训练后是否能够通过优化实现表示对齐，以验证柏拉图表示假设。

Method: 引入JAM框架，联合训练模态特定的自动编码器，结合重构和跨模态目标，优化表示对齐。

Result: JAM框架在多种对齐目标、层深度和基础模型规模下均能有效诱导表示对齐。

Conclusion: 该框架为将通用单模态基础模型转化为专用多模态模型提供了理论和实践路径。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [233] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: 论文提出了一种名为SODA的算法，用于从大型语言模型（LLM）的输出中精确重构输入，解决了现有审计技术未能覆盖的取证问题。


<details>
  <summary>Details</summary>
Motivation: 现有审计技术主要关注识别LLM中的潜在不良行为，而本文则专注于解决如何从现有输出中重构原始输入的取证问题，以支持事后分析和检测虚假报告。

Method: 将精确输入重构问题形式化为离散优化问题，并引入SODA算法，该算法基于梯度优化，在连续松弛的输入搜索空间中运行，采用周期性重启和参数衰减策略。

Result: 在33M到3B参数的LLM上进行实验，SODA显著优于现有方法，成功恢复了79.5%的短分布外输入，但对长序列（15+ token）的隐私信息提取效果不佳。

Conclusion: 标准部署实践可能已足够防止恶意使用SODA方法，但该方法在短输入重构上表现出色，为LLM的取证分析提供了新工具。

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [234] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: RelFCI是一种针对具有潜在混杂因素的关系数据的因果发现算法，填补了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现实世界的关系数据中因果效应估计困难，现有方法无法处理潜在混杂因素或非独立同分布数据。

Method: 基于FCI和RCD算法，提出RelFCI，定义新的图模型以支持关系领域的因果发现，并建立关系d-分离的理论保证。

Result: 实验证明RelFCI能有效识别具有潜在混杂因素的关系因果模型中的正确因果结构。

Conclusion: RelFCI为关系数据中的因果发现提供了可靠且完整的解决方案。

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [235] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: 研究探讨了神经网络最后一层权重重采样（“zapping”）在持续学习和少样本迁移学习中的效果及其机制。


<details>
  <summary>Details</summary>
Motivation: 尽管实证结果显示zapping有效，但其背后的机制尚不明确，因此需要深入研究。

Method: 在卷积神经网络中，通过手写字符和自然图像的实验，分析持续学习和少样本迁移学习中的学习与遗忘模式。

Result: 实验表明，zapping训练的模型能更快适应新领域；优化器选择也会影响学习与遗忘的动态。

Conclusion: zapping和优化器选择共同影响任务间的协同/干扰模式，揭示了持续学习的复杂性。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [236] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: 论文提出了两种新指标Clipped Density和Clipped Coverage，用于更可靠、可解释地评估生成模型样本质量，解决了现有指标在鲁棒性和校准方面的不足。


<details>
  <summary>Details</summary>
Motivation: 生成模型在关键应用中的使用受到样本质量评估不可靠的限制，现有指标缺乏校准或对异常值的鲁棒性。

Method: 通过剪裁单个样本贡献和最近邻球的半径，提出Clipped Density和Clipped Coverage指标，防止异常样本影响整体评估。

Result: 实验表明，新指标在鲁棒性、敏感性和可解释性上优于现有方法。

Conclusion: Clipped Density和Clipped Coverage为生成模型样本质量评估提供了更可靠的解决方案。

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [237] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet将决策树集成转换为稀疏、部分连接的神经网络，保留符号结构并支持梯度优化，性能优于XGBoost。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的梯度优化能力与决策树的符号可解释性，构建紧凑且无需手动调优的模型。

Method: 将决策树的每个分支映射为隐藏神经元，形成稀疏神经网络，保留符号结构。

Result: 在多类分类任务中，BranchNet准确率显著优于XGBoost。

Conclusion: BranchNet在符号可解释性和性能上表现优异，但在二元任务中可能需要进一步优化。

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [238] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: 提出一种新方法，利用GNN解决SAT问题，通过将k-CNF公式映射为MILP问题并编码为加权二分图，再输入GNN训练和测试。理论证明其稳定性和局限性，实验显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 探索GNN在解决SAT问题中的应用，结合MILP技术，提升解决效率。

Method: 将k-CNF公式映射为MILP问题，编码为加权二分图后输入GNN。

Result: 理论证明方法的稳定性和局限性，实验显示其有效性。

Conclusion: 该方法在SAT问题上表现良好，具有理论和实践价值。

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [239] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: mGRADE是一种混合内存系统，结合了1D卷积和最小门控循环单元，适用于内存受限的边缘设备的多尺度时间处理。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要同时捕捉短时和长时动态的模型，但现有方法（如Transformer、RNN、TCN）在内存或效率上存在不足。

Method: 提出mGRADE，结合可学习间隔的1D卷积和minGRU，卷积层捕捉快速变化，循环模块维护全局上下文。

Result: 在合成任务和图像分类基准测试中，mGRADE优于纯卷积和纯循环模型，内存占用减少20%。

Conclusion: mGRADE是内存受限边缘设备中多尺度时间处理的高效解决方案。

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [240] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González,Pedro Casas,Emilio Martínez,Alicia Fernández*

Main category: cs.LG

TL;DR: FAE是一种基于变分自编码器（VAE）和扩张卷积神经网络（DCNN）的基础生成AI模型，用于时间序列数据的异常检测。


<details>
  <summary>Details</summary>
Motivation: 受大型预训练基础模型成功的启发，研究旨在开发一种通用的时间序列建模方法，适用于零样本异常检测。

Method: 结合VAE和DCNN构建FAE模型，利用大规模时间序列数据进行预训练。

Result: 在多个领域的时间序列数据集（包括移动ISP数据和KDD 2021数据集）上展示了初步结果。

Conclusion: FAE为时间序列建模和异常检测提供了一种通用的基础模型，具有潜在的应用价值。

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [241] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: 论文提出了一种结合LSTM和Transformer的混合深度学习模型，利用iForest和AE进行伪标记，用于心理健康账单异常检测，解决了类别不平衡和标签稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康账单的复杂性导致异常（如欺诈）频发，而传统机器学习方法在类别不平衡、标签稀缺和复杂序列模式上表现不佳。

Method: 采用混合深度学习模型（LSTM和Transformer），结合iForest和AE进行伪标记，并在两个真实心理健康账单数据集上评估。

Result: iForest LSTM基线在声明级数据上召回率最高（0.963）；在操作级数据上，混合iForest模型召回率最高（0.744），但精度较低。

Conclusion: 研究表明，伪标记与混合深度学习结合在复杂不平衡的异常检测中具有潜力。

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [242] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Main category: eess.AS

TL;DR: 本研究通过评估八种开源音乐生成系统（MGS），探讨其在现代音乐制作中的实用性，提出了一种结合技术与实践的评估框架，发现MGS更多是辅助工具而非替代人类创造力。


<details>
  <summary>Details</summary>
Motivation: 探索音乐生成系统在现代音乐制作中的实际应用潜力，填补当前系统在主题和结构连贯性上的不足。

Method: 采用单评估者混合方法，结合定性假设和定量指标，评估八种具有架构多样性的开源MGS。

Result: MGS主要作为辅助工具增强人类创造力，但在情感深度和复杂决策任务中仍依赖人类。

Conclusion: 研究提出了一个结构化评估框架，为未来MGS开发和AI在创意工作流中的整合提供了实证指导。

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


### [243] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/abs/2507.01024)
*George Igwegbe,Martins Awojide,Mboh Bless,Nirel Kadzo*

Main category: eess.AS

TL;DR: Hello Afrika项目旨在解决非洲语言语音命令模型的不足，首阶段专注于基尼亚卢旺达语，构建了一个包含通用指令、数字和唤醒词的语音命令模型，并在多种设备上部署和评估。


<details>
  <summary>Details</summary>
Motivation: 非洲语言语音命令模型匮乏，基尼亚卢旺达语因该国对语音识别技术的兴趣及数据集规模成为首个目标。

Method: 基于自定义语音命令语料库（含通用指令、数字和唤醒词）构建模型，并在PC、手机和边缘设备上部署。

Result: 模型在多种设备上部署，并通过适当指标评估性能。

Conclusion: Hello Afrika项目成功填补了基尼亚卢旺达语语音命令模型的空白，展示了多设备部署的可行性。

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [244] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Main category: eess.AS

TL;DR: 提出了一种开源框架，用于命令式听写，填补了资源密集型在线系统与高延迟批处理之间的空白。


<details>
  <summary>Details</summary>
Motivation: 解决现有听写系统中资源密集和高延迟的问题，提供一个高效且兼容多种ASR架构的解决方案。

Method: 使用语音活动检测（VAD）分割音频，并利用Whisper模型并行转录，支持多音频复用。

Result: 在印度约15%的法庭中部署，实时数据显示随着用户并发增加，延迟显著降低。

Conclusion: 该框架高效、开源且兼容性强，适用于实际场景，展示了显著的性能提升。

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [245] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Main category: math.NA

TL;DR: 本文提出了一种基于GPU的全局搜索方法，利用区间分析和GPU计算能力，确保在非线性函数中找到全局最小值。


<details>
  <summary>Details</summary>
Motivation: 解决高维非线性函数的全局优化问题，尤其是在存在舍入误差时仍能保证结果的准确性。

Method: 结合区间分析和GPU并行计算，通过迭代排除不可能存在全局最小值的区域，并采用单程序多数据（SPMD）编程风格和变量循环技术提高效率。

Result: 成功在10个多模态基准测试函数（包括Ackley、Griewank等）中，最高达到10,000维的情况下，找到全局最小值。

Conclusion: 该方法在高维全局优化问题中表现出色，显著优于文献中的现有结果。

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [246] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Main category: cs.DC

TL;DR: 该论文提出了一种混合云和本地服务器的资源容量规划与作业调度方法，重点处理资源使用和作业持续时间的不确定性，以平衡资源最小化和服务质量。


<details>
  <summary>Details</summary>
Motivation: 随着云计算基础设施的普及，组织需要同时管理云和本地服务器资源。金融行业中，市场条件的不确定性对作业特性影响显著，因此需要一种能处理不确定性的容量规划和调度方法。

Method: 采用确定性估计器和基于配对采样的约束编程方法，同时优化资源使用和服务质量。

Result: 基于配对采样的方法在降低峰值资源使用的同时，未牺牲服务质量，优于手动调度。

Conclusion: 该方法在金融行业等不确定性高的环境中，有效平衡了资源使用和服务质量，具有实际应用价值。

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [247] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Main category: cs.DC

TL;DR: EdgeLoRA是一个高效系统，用于在多租户边缘设备上部署大型语言模型（LLMs），通过自适应适配器选择、异构内存管理和批量LoRA推理，显著提升性能和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在多租户边缘设备上高效部署LLMs面临适配器选择复杂、内存开销大和计算资源利用率低等挑战。

Method: EdgeLoRA引入自适应适配器选择、异构内存管理和批量LoRA推理三项创新技术。

Result: 实验表明，EdgeLoRA在延迟和吞吐量上显著优于现有方案（如llama.cpp），吞吐量提升高达4倍，同时支持更多适配器。

Conclusion: EdgeLoRA为资源受限环境中的LLMs边缘部署提供了可扩展且高效的解决方案。

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


### [248] [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](https://arxiv.org/abs/2507.01676)
*Giuseppe Ruggeri,Renzo Andri,Daniele Jahier Pagliari,Lukas Cavigelli*

Main category: cs.DC

TL;DR: 提出了一种针对深度推荐模型（DLRM）嵌入层性能瓶颈的优化方法，通过定制数据流和自动映射框架，显著提升了嵌入查找速度。


<details>
  <summary>Details</summary>
Motivation: DLRM在Meta数据中心占AI工作负载的79%以上，其性能瓶颈主要在于嵌入层的随机内存访问。

Method: 设计了四种单核嵌入表查找策略，并提出一个框架将表非对称映射到SoC多核上。

Result: 在华为Ascend AI加速器上测试，速度提升1.5x至6.5x，极端不平衡分布下可达20x以上。

Conclusion: 该方法显著提升了嵌入查找效率，且对查询分布的依赖性远低于基线方法。

Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

</details>
